{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a13665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0789b028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==============================\n",
      " Encoder Loss: Triplet_L2\n",
      "==============================\n",
      "\n",
      "----- SPLIT 1 -----\n",
      "Epoch 001 | Val Acc: 0.0282\n",
      "Epoch 002 | Val Acc: 0.2153\n",
      "Epoch 003 | Val Acc: 0.3935\n",
      "Epoch 004 | Val Acc: 0.5298\n",
      "Epoch 005 | Val Acc: 0.6097\n",
      "Epoch 006 | Val Acc: 0.6468\n",
      "Epoch 007 | Val Acc: 0.6911\n",
      "Epoch 008 | Val Acc: 0.7129\n",
      "Epoch 009 | Val Acc: 0.6976\n",
      "Epoch 010 | Val Acc: 0.7008\n",
      "Epoch 011 | Val Acc: 0.7315\n",
      "Epoch 012 | Val Acc: 0.7411\n",
      "Epoch 013 | Val Acc: 0.7363\n",
      "Epoch 014 | Val Acc: 0.7468\n",
      "Epoch 015 | Val Acc: 0.7492\n",
      "Epoch 016 | Val Acc: 0.7468\n",
      "Epoch 017 | Val Acc: 0.7484\n",
      "Epoch 018 | Val Acc: 0.7621\n",
      "Epoch 019 | Val Acc: 0.7468\n",
      "Epoch 020 | Val Acc: 0.7661\n",
      "Epoch 021 | Val Acc: 0.7581\n",
      "Epoch 022 | Val Acc: 0.7661\n",
      "Epoch 023 | Val Acc: 0.7790\n",
      "Epoch 024 | Val Acc: 0.7782\n",
      "Epoch 025 | Val Acc: 0.7750\n",
      "Epoch 026 | Val Acc: 0.7492\n",
      "Epoch 027 | Val Acc: 0.7629\n",
      "Epoch 028 | Val Acc: 0.7782\n",
      "Epoch 029 | Val Acc: 0.7742\n",
      "Epoch 030 | Val Acc: 0.7621\n",
      "Epoch 031 | Val Acc: 0.7637\n",
      "Epoch 032 | Val Acc: 0.7419\n",
      "Epoch 033 | Val Acc: 0.7718\n",
      "Early stopping\n",
      "Split Accuracy: 0.7790\n",
      "\n",
      "----- SPLIT 2 -----\n",
      "Epoch 001 | Val Acc: 0.0290\n",
      "Epoch 002 | Val Acc: 0.2129\n",
      "Epoch 003 | Val Acc: 0.4621\n",
      "Epoch 004 | Val Acc: 0.5460\n",
      "Epoch 005 | Val Acc: 0.6363\n",
      "Epoch 006 | Val Acc: 0.6597\n",
      "Epoch 007 | Val Acc: 0.7226\n",
      "Epoch 008 | Val Acc: 0.7234\n",
      "Epoch 009 | Val Acc: 0.7645\n",
      "Epoch 010 | Val Acc: 0.7573\n",
      "Epoch 011 | Val Acc: 0.7565\n",
      "Epoch 012 | Val Acc: 0.7782\n",
      "Epoch 013 | Val Acc: 0.7790\n",
      "Epoch 014 | Val Acc: 0.7944\n",
      "Epoch 015 | Val Acc: 0.8081\n",
      "Epoch 016 | Val Acc: 0.7992\n",
      "Epoch 017 | Val Acc: 0.8194\n",
      "Epoch 018 | Val Acc: 0.8169\n",
      "Epoch 019 | Val Acc: 0.8024\n",
      "Epoch 020 | Val Acc: 0.8137\n",
      "Epoch 021 | Val Acc: 0.8137\n",
      "Epoch 022 | Val Acc: 0.8258\n",
      "Epoch 023 | Val Acc: 0.8306\n",
      "Epoch 024 | Val Acc: 0.8153\n",
      "Epoch 025 | Val Acc: 0.8258\n",
      "Epoch 026 | Val Acc: 0.8145\n",
      "Epoch 027 | Val Acc: 0.8371\n",
      "Epoch 028 | Val Acc: 0.8323\n",
      "Epoch 029 | Val Acc: 0.8282\n",
      "Epoch 030 | Val Acc: 0.8282\n",
      "Epoch 031 | Val Acc: 0.8315\n",
      "Epoch 032 | Val Acc: 0.8298\n",
      "Epoch 033 | Val Acc: 0.8452\n",
      "Epoch 034 | Val Acc: 0.8629\n",
      "Epoch 035 | Val Acc: 0.8379\n",
      "Epoch 036 | Val Acc: 0.8290\n",
      "Epoch 037 | Val Acc: 0.8363\n",
      "Epoch 038 | Val Acc: 0.8323\n",
      "Epoch 039 | Val Acc: 0.8194\n",
      "Epoch 040 | Val Acc: 0.8500\n",
      "Epoch 041 | Val Acc: 0.8597\n",
      "Epoch 042 | Val Acc: 0.8524\n",
      "Epoch 043 | Val Acc: 0.8573\n",
      "Epoch 044 | Val Acc: 0.8516\n",
      "Early stopping\n",
      "Split Accuracy: 0.8629\n",
      "\n",
      "----- SPLIT 3 -----\n",
      "Epoch 001 | Val Acc: 0.0194\n",
      "Epoch 002 | Val Acc: 0.0871\n",
      "Epoch 003 | Val Acc: 0.3137\n",
      "Epoch 004 | Val Acc: 0.4427\n",
      "Epoch 005 | Val Acc: 0.5766\n",
      "Epoch 006 | Val Acc: 0.6290\n",
      "Epoch 007 | Val Acc: 0.6726\n",
      "Epoch 008 | Val Acc: 0.7177\n",
      "Epoch 009 | Val Acc: 0.7258\n",
      "Epoch 010 | Val Acc: 0.7613\n",
      "Epoch 011 | Val Acc: 0.7508\n",
      "Epoch 012 | Val Acc: 0.7613\n",
      "Epoch 013 | Val Acc: 0.7782\n",
      "Epoch 014 | Val Acc: 0.7863\n",
      "Epoch 015 | Val Acc: 0.7798\n",
      "Epoch 016 | Val Acc: 0.7935\n",
      "Epoch 017 | Val Acc: 0.7879\n",
      "Epoch 018 | Val Acc: 0.7911\n",
      "Epoch 019 | Val Acc: 0.7976\n",
      "Epoch 020 | Val Acc: 0.7960\n",
      "Epoch 021 | Val Acc: 0.8097\n",
      "Epoch 022 | Val Acc: 0.7992\n",
      "Epoch 023 | Val Acc: 0.8065\n",
      "Epoch 024 | Val Acc: 0.8097\n",
      "Epoch 025 | Val Acc: 0.8129\n",
      "Epoch 026 | Val Acc: 0.8347\n",
      "Epoch 027 | Val Acc: 0.8194\n",
      "Epoch 028 | Val Acc: 0.8234\n",
      "Epoch 029 | Val Acc: 0.8218\n",
      "Epoch 030 | Val Acc: 0.8129\n",
      "Epoch 031 | Val Acc: 0.8113\n",
      "Epoch 032 | Val Acc: 0.8242\n",
      "Epoch 033 | Val Acc: 0.8266\n",
      "Epoch 034 | Val Acc: 0.8347\n",
      "Epoch 035 | Val Acc: 0.8419\n",
      "Epoch 036 | Val Acc: 0.8419\n",
      "Epoch 037 | Val Acc: 0.8476\n",
      "Epoch 038 | Val Acc: 0.8242\n",
      "Epoch 039 | Val Acc: 0.8202\n",
      "Epoch 040 | Val Acc: 0.8177\n",
      "Epoch 041 | Val Acc: 0.8452\n",
      "Epoch 042 | Val Acc: 0.8573\n",
      "Epoch 043 | Val Acc: 0.8484\n",
      "Epoch 044 | Val Acc: 0.8548\n",
      "Epoch 045 | Val Acc: 0.8548\n",
      "Epoch 046 | Val Acc: 0.8444\n",
      "Epoch 047 | Val Acc: 0.8500\n",
      "Epoch 048 | Val Acc: 0.8387\n",
      "Epoch 049 | Val Acc: 0.8419\n",
      "Epoch 050 | Val Acc: 0.8452\n",
      "Epoch 051 | Val Acc: 0.8508\n",
      "Epoch 052 | Val Acc: 0.8290\n",
      "Early stopping\n",
      "Split Accuracy: 0.8573\n",
      "\n",
      "----- SPLIT 4 -----\n",
      "Epoch 001 | Val Acc: 0.0282\n",
      "Epoch 002 | Val Acc: 0.2008\n",
      "Epoch 003 | Val Acc: 0.4444\n",
      "Epoch 004 | Val Acc: 0.5371\n",
      "Epoch 005 | Val Acc: 0.6298\n",
      "Epoch 006 | Val Acc: 0.7097\n",
      "Epoch 007 | Val Acc: 0.7363\n",
      "Epoch 008 | Val Acc: 0.7306\n",
      "Epoch 009 | Val Acc: 0.7815\n",
      "Epoch 010 | Val Acc: 0.7960\n",
      "Epoch 011 | Val Acc: 0.8032\n",
      "Epoch 012 | Val Acc: 0.7919\n",
      "Epoch 013 | Val Acc: 0.8065\n",
      "Epoch 014 | Val Acc: 0.8089\n",
      "Epoch 015 | Val Acc: 0.8282\n",
      "Epoch 016 | Val Acc: 0.8290\n",
      "Epoch 017 | Val Acc: 0.8419\n",
      "Epoch 018 | Val Acc: 0.8306\n",
      "Epoch 019 | Val Acc: 0.8363\n",
      "Epoch 020 | Val Acc: 0.8298\n",
      "Epoch 021 | Val Acc: 0.8476\n",
      "Epoch 022 | Val Acc: 0.8492\n",
      "Epoch 023 | Val Acc: 0.8548\n",
      "Epoch 024 | Val Acc: 0.8516\n",
      "Epoch 025 | Val Acc: 0.8379\n",
      "Epoch 026 | Val Acc: 0.8508\n",
      "Epoch 027 | Val Acc: 0.8444\n",
      "Epoch 028 | Val Acc: 0.8516\n",
      "Epoch 029 | Val Acc: 0.8565\n",
      "Epoch 030 | Val Acc: 0.8540\n",
      "Epoch 031 | Val Acc: 0.8573\n",
      "Epoch 032 | Val Acc: 0.8113\n",
      "Epoch 033 | Val Acc: 0.8500\n",
      "Epoch 034 | Val Acc: 0.8492\n",
      "Epoch 035 | Val Acc: 0.8653\n",
      "Epoch 036 | Val Acc: 0.8669\n",
      "Epoch 037 | Val Acc: 0.8734\n",
      "Epoch 038 | Val Acc: 0.8774\n",
      "Epoch 039 | Val Acc: 0.8766\n",
      "Epoch 040 | Val Acc: 0.8685\n",
      "Epoch 041 | Val Acc: 0.8669\n",
      "Epoch 042 | Val Acc: 0.8653\n",
      "Epoch 043 | Val Acc: 0.8613\n",
      "Epoch 044 | Val Acc: 0.8387\n",
      "Epoch 045 | Val Acc: 0.8661\n",
      "Epoch 046 | Val Acc: 0.8573\n",
      "Epoch 047 | Val Acc: 0.8669\n",
      "Epoch 048 | Val Acc: 0.8718\n",
      "Early stopping\n",
      "Split Accuracy: 0.8774\n",
      "\n",
      "----- SPLIT 5 -----\n",
      "Epoch 001 | Val Acc: 0.0323\n",
      "Epoch 002 | Val Acc: 0.0645\n",
      "Epoch 003 | Val Acc: 0.3460\n",
      "Epoch 004 | Val Acc: 0.4734\n",
      "Epoch 005 | Val Acc: 0.5895\n",
      "Epoch 006 | Val Acc: 0.6484\n",
      "Epoch 007 | Val Acc: 0.7081\n",
      "Epoch 008 | Val Acc: 0.7129\n",
      "Epoch 009 | Val Acc: 0.7621\n",
      "Epoch 010 | Val Acc: 0.7492\n",
      "Epoch 011 | Val Acc: 0.7774\n",
      "Epoch 012 | Val Acc: 0.8008\n",
      "Epoch 013 | Val Acc: 0.7863\n",
      "Epoch 014 | Val Acc: 0.7968\n",
      "Epoch 015 | Val Acc: 0.8040\n",
      "Epoch 016 | Val Acc: 0.8185\n",
      "Epoch 017 | Val Acc: 0.8371\n",
      "Epoch 018 | Val Acc: 0.8097\n",
      "Epoch 019 | Val Acc: 0.8218\n",
      "Epoch 020 | Val Acc: 0.8081\n",
      "Epoch 021 | Val Acc: 0.8379\n",
      "Epoch 022 | Val Acc: 0.8468\n",
      "Epoch 023 | Val Acc: 0.8444\n",
      "Epoch 024 | Val Acc: 0.8371\n",
      "Epoch 025 | Val Acc: 0.8452\n",
      "Epoch 026 | Val Acc: 0.8315\n",
      "Epoch 027 | Val Acc: 0.8524\n",
      "Epoch 028 | Val Acc: 0.8710\n",
      "Epoch 029 | Val Acc: 0.8685\n",
      "Epoch 030 | Val Acc: 0.8677\n",
      "Epoch 031 | Val Acc: 0.8419\n",
      "Epoch 032 | Val Acc: 0.8379\n",
      "Epoch 033 | Val Acc: 0.8419\n",
      "Epoch 034 | Val Acc: 0.8565\n",
      "Epoch 035 | Val Acc: 0.8734\n",
      "Epoch 036 | Val Acc: 0.8532\n",
      "Epoch 037 | Val Acc: 0.8645\n",
      "Epoch 038 | Val Acc: 0.8806\n",
      "Epoch 039 | Val Acc: 0.8742\n",
      "Epoch 040 | Val Acc: 0.8798\n",
      "Epoch 041 | Val Acc: 0.8871\n",
      "Epoch 042 | Val Acc: 0.8774\n",
      "Epoch 043 | Val Acc: 0.8460\n",
      "Epoch 044 | Val Acc: 0.8694\n",
      "Epoch 045 | Val Acc: 0.8565\n",
      "Epoch 046 | Val Acc: 0.8766\n",
      "Epoch 047 | Val Acc: 0.8653\n",
      "Epoch 048 | Val Acc: 0.8718\n",
      "Epoch 049 | Val Acc: 0.8677\n",
      "Epoch 050 | Val Acc: 0.8815\n",
      "Epoch 051 | Val Acc: 0.8895\n",
      "Epoch 052 | Val Acc: 0.8863\n",
      "Epoch 053 | Val Acc: 0.8839\n",
      "Epoch 054 | Val Acc: 0.8879\n",
      "Epoch 055 | Val Acc: 0.8863\n",
      "Epoch 056 | Val Acc: 0.8306\n",
      "Epoch 057 | Val Acc: 0.8556\n",
      "Epoch 058 | Val Acc: 0.8815\n",
      "Epoch 059 | Val Acc: 0.8911\n",
      "Epoch 060 | Val Acc: 0.8944\n",
      "Epoch 061 | Val Acc: 0.8984\n",
      "Epoch 062 | Val Acc: 0.9016\n",
      "Epoch 063 | Val Acc: 0.8968\n",
      "Epoch 064 | Val Acc: 0.8726\n",
      "Epoch 065 | Val Acc: 0.8508\n",
      "Epoch 066 | Val Acc: 0.8589\n",
      "Epoch 067 | Val Acc: 0.8718\n",
      "Epoch 068 | Val Acc: 0.8879\n",
      "Epoch 069 | Val Acc: 0.8903\n",
      "Epoch 070 | Val Acc: 0.8879\n",
      "Epoch 071 | Val Acc: 0.8911\n",
      "Epoch 072 | Val Acc: 0.8855\n",
      "Early stopping\n",
      "Split Accuracy: 0.9016\n",
      "\n",
      "----- SPLIT 6 -----\n",
      "Epoch 001 | Val Acc: 0.0274\n",
      "Epoch 002 | Val Acc: 0.1056\n",
      "Epoch 003 | Val Acc: 0.3089\n",
      "Epoch 004 | Val Acc: 0.5000\n",
      "Epoch 005 | Val Acc: 0.5968\n",
      "Epoch 006 | Val Acc: 0.6621\n",
      "Epoch 007 | Val Acc: 0.7048\n",
      "Epoch 008 | Val Acc: 0.7605\n",
      "Epoch 009 | Val Acc: 0.7629\n",
      "Epoch 010 | Val Acc: 0.7815\n",
      "Epoch 011 | Val Acc: 0.7879\n",
      "Epoch 012 | Val Acc: 0.8040\n",
      "Epoch 013 | Val Acc: 0.8089\n",
      "Epoch 014 | Val Acc: 0.8282\n",
      "Epoch 015 | Val Acc: 0.8065\n",
      "Epoch 016 | Val Acc: 0.8363\n",
      "Epoch 017 | Val Acc: 0.8145\n",
      "Epoch 018 | Val Acc: 0.8331\n",
      "Epoch 019 | Val Acc: 0.8419\n",
      "Epoch 020 | Val Acc: 0.8379\n",
      "Epoch 021 | Val Acc: 0.8339\n",
      "Epoch 022 | Val Acc: 0.8266\n",
      "Epoch 023 | Val Acc: 0.8548\n",
      "Epoch 024 | Val Acc: 0.8476\n",
      "Epoch 025 | Val Acc: 0.8419\n",
      "Epoch 026 | Val Acc: 0.8476\n",
      "Epoch 027 | Val Acc: 0.8427\n",
      "Epoch 028 | Val Acc: 0.8226\n",
      "Epoch 029 | Val Acc: 0.8460\n",
      "Epoch 030 | Val Acc: 0.8637\n",
      "Epoch 031 | Val Acc: 0.8637\n",
      "Epoch 032 | Val Acc: 0.8694\n",
      "Epoch 033 | Val Acc: 0.8540\n",
      "Epoch 034 | Val Acc: 0.8589\n",
      "Epoch 035 | Val Acc: 0.8694\n",
      "Epoch 036 | Val Acc: 0.8621\n",
      "Epoch 037 | Val Acc: 0.8306\n",
      "Epoch 038 | Val Acc: 0.8282\n",
      "Epoch 039 | Val Acc: 0.8355\n",
      "Epoch 040 | Val Acc: 0.8403\n",
      "Epoch 041 | Val Acc: 0.8565\n",
      "Epoch 042 | Val Acc: 0.8589\n",
      "Early stopping\n",
      "Split Accuracy: 0.8694\n",
      "\n",
      "----- SPLIT 7 -----\n",
      "Epoch 001 | Val Acc: 0.0395\n",
      "Epoch 002 | Val Acc: 0.1750\n",
      "Epoch 003 | Val Acc: 0.2887\n",
      "Epoch 004 | Val Acc: 0.4702\n",
      "Epoch 005 | Val Acc: 0.5524\n",
      "Epoch 006 | Val Acc: 0.6492\n",
      "Epoch 007 | Val Acc: 0.6911\n",
      "Epoch 008 | Val Acc: 0.7194\n",
      "Epoch 009 | Val Acc: 0.7371\n",
      "Epoch 010 | Val Acc: 0.7621\n",
      "Epoch 011 | Val Acc: 0.7734\n",
      "Epoch 012 | Val Acc: 0.7734\n",
      "Epoch 013 | Val Acc: 0.7952\n",
      "Epoch 014 | Val Acc: 0.7806\n",
      "Epoch 015 | Val Acc: 0.8161\n",
      "Epoch 016 | Val Acc: 0.8048\n",
      "Epoch 017 | Val Acc: 0.8105\n",
      "Epoch 018 | Val Acc: 0.8290\n",
      "Epoch 019 | Val Acc: 0.8210\n",
      "Epoch 020 | Val Acc: 0.8379\n",
      "Epoch 021 | Val Acc: 0.8516\n",
      "Epoch 022 | Val Acc: 0.8129\n",
      "Epoch 023 | Val Acc: 0.8484\n",
      "Epoch 024 | Val Acc: 0.8274\n",
      "Epoch 025 | Val Acc: 0.8629\n",
      "Epoch 026 | Val Acc: 0.8484\n",
      "Epoch 027 | Val Acc: 0.8403\n",
      "Epoch 028 | Val Acc: 0.8476\n",
      "Epoch 029 | Val Acc: 0.8355\n",
      "Epoch 030 | Val Acc: 0.8565\n",
      "Epoch 031 | Val Acc: 0.8589\n",
      "Epoch 032 | Val Acc: 0.8363\n",
      "Epoch 033 | Val Acc: 0.8508\n",
      "Epoch 034 | Val Acc: 0.8484\n",
      "Epoch 035 | Val Acc: 0.8540\n",
      "Early stopping\n",
      "Split Accuracy: 0.8629\n",
      "\n",
      "----- SPLIT 8 -----\n",
      "Epoch 001 | Val Acc: 0.0524\n",
      "Epoch 002 | Val Acc: 0.2976\n",
      "Epoch 003 | Val Acc: 0.4508\n",
      "Epoch 004 | Val Acc: 0.5702\n",
      "Epoch 005 | Val Acc: 0.6282\n",
      "Epoch 006 | Val Acc: 0.7121\n",
      "Epoch 007 | Val Acc: 0.7266\n",
      "Epoch 008 | Val Acc: 0.7427\n",
      "Epoch 009 | Val Acc: 0.7508\n",
      "Epoch 010 | Val Acc: 0.7718\n",
      "Epoch 011 | Val Acc: 0.8040\n",
      "Epoch 012 | Val Acc: 0.7919\n",
      "Epoch 013 | Val Acc: 0.8129\n",
      "Epoch 014 | Val Acc: 0.8250\n",
      "Epoch 015 | Val Acc: 0.8306\n",
      "Epoch 016 | Val Acc: 0.8000\n",
      "Epoch 017 | Val Acc: 0.8250\n",
      "Epoch 018 | Val Acc: 0.8379\n",
      "Epoch 019 | Val Acc: 0.8169\n",
      "Epoch 020 | Val Acc: 0.8266\n",
      "Epoch 021 | Val Acc: 0.8419\n",
      "Epoch 022 | Val Acc: 0.8395\n",
      "Epoch 023 | Val Acc: 0.8105\n",
      "Epoch 024 | Val Acc: 0.8266\n",
      "Epoch 025 | Val Acc: 0.8347\n",
      "Epoch 026 | Val Acc: 0.8258\n",
      "Epoch 027 | Val Acc: 0.8315\n",
      "Epoch 028 | Val Acc: 0.8532\n",
      "Epoch 029 | Val Acc: 0.8403\n",
      "Epoch 030 | Val Acc: 0.8532\n",
      "Epoch 031 | Val Acc: 0.8540\n",
      "Epoch 032 | Val Acc: 0.8371\n",
      "Epoch 033 | Val Acc: 0.8395\n",
      "Epoch 034 | Val Acc: 0.8242\n",
      "Epoch 035 | Val Acc: 0.8371\n",
      "Epoch 036 | Val Acc: 0.8500\n",
      "Epoch 037 | Val Acc: 0.8508\n",
      "Epoch 038 | Val Acc: 0.8548\n",
      "Epoch 039 | Val Acc: 0.8629\n",
      "Epoch 040 | Val Acc: 0.8613\n",
      "Epoch 041 | Val Acc: 0.8621\n",
      "Epoch 042 | Val Acc: 0.8661\n",
      "Epoch 043 | Val Acc: 0.8629\n",
      "Epoch 044 | Val Acc: 0.8605\n",
      "Epoch 045 | Val Acc: 0.8556\n",
      "Epoch 046 | Val Acc: 0.8661\n",
      "Epoch 047 | Val Acc: 0.8685\n",
      "Epoch 048 | Val Acc: 0.8540\n",
      "Epoch 049 | Val Acc: 0.8282\n",
      "Epoch 050 | Val Acc: 0.8065\n",
      "Epoch 051 | Val Acc: 0.8508\n",
      "Epoch 052 | Val Acc: 0.8613\n",
      "Epoch 053 | Val Acc: 0.8653\n",
      "Epoch 054 | Val Acc: 0.8694\n",
      "Epoch 055 | Val Acc: 0.8702\n",
      "Epoch 056 | Val Acc: 0.8702\n",
      "Epoch 057 | Val Acc: 0.8758\n",
      "Epoch 058 | Val Acc: 0.8726\n",
      "Epoch 059 | Val Acc: 0.8621\n",
      "Epoch 060 | Val Acc: 0.8694\n",
      "Epoch 061 | Val Acc: 0.8694\n",
      "Epoch 062 | Val Acc: 0.8661\n",
      "Epoch 063 | Val Acc: 0.8742\n",
      "Epoch 064 | Val Acc: 0.8669\n",
      "Epoch 065 | Val Acc: 0.8653\n",
      "Epoch 066 | Val Acc: 0.8742\n",
      "Epoch 067 | Val Acc: 0.8629\n",
      "Early stopping\n",
      "Split Accuracy: 0.8758\n",
      "\n",
      "----- SPLIT 9 -----\n",
      "Epoch 001 | Val Acc: 0.0395\n",
      "Epoch 002 | Val Acc: 0.1887\n",
      "Epoch 003 | Val Acc: 0.4121\n",
      "Epoch 004 | Val Acc: 0.5040\n",
      "Epoch 005 | Val Acc: 0.5718\n",
      "Epoch 006 | Val Acc: 0.6581\n",
      "Epoch 007 | Val Acc: 0.6669\n",
      "Epoch 008 | Val Acc: 0.7355\n",
      "Epoch 009 | Val Acc: 0.7556\n",
      "Epoch 010 | Val Acc: 0.7823\n",
      "Epoch 011 | Val Acc: 0.7887\n",
      "Epoch 012 | Val Acc: 0.7903\n",
      "Epoch 013 | Val Acc: 0.7935\n",
      "Epoch 014 | Val Acc: 0.8169\n",
      "Epoch 015 | Val Acc: 0.8161\n",
      "Epoch 016 | Val Acc: 0.8387\n",
      "Epoch 017 | Val Acc: 0.8282\n",
      "Epoch 018 | Val Acc: 0.8282\n",
      "Epoch 019 | Val Acc: 0.8476\n",
      "Epoch 020 | Val Acc: 0.8508\n",
      "Epoch 021 | Val Acc: 0.8411\n",
      "Epoch 022 | Val Acc: 0.8347\n",
      "Epoch 023 | Val Acc: 0.8452\n",
      "Epoch 024 | Val Acc: 0.8524\n",
      "Epoch 025 | Val Acc: 0.8661\n",
      "Epoch 026 | Val Acc: 0.8532\n",
      "Epoch 027 | Val Acc: 0.8597\n",
      "Epoch 028 | Val Acc: 0.8581\n",
      "Epoch 029 | Val Acc: 0.8613\n",
      "Epoch 030 | Val Acc: 0.8677\n",
      "Epoch 031 | Val Acc: 0.8734\n",
      "Epoch 032 | Val Acc: 0.8605\n",
      "Epoch 033 | Val Acc: 0.8556\n",
      "Epoch 034 | Val Acc: 0.8508\n",
      "Epoch 035 | Val Acc: 0.8565\n",
      "Epoch 036 | Val Acc: 0.8589\n",
      "Epoch 037 | Val Acc: 0.8581\n",
      "Epoch 038 | Val Acc: 0.8750\n",
      "Epoch 039 | Val Acc: 0.8718\n",
      "Epoch 040 | Val Acc: 0.8742\n",
      "Epoch 041 | Val Acc: 0.8782\n",
      "Epoch 042 | Val Acc: 0.8774\n",
      "Epoch 043 | Val Acc: 0.8815\n",
      "Epoch 044 | Val Acc: 0.8903\n",
      "Epoch 045 | Val Acc: 0.8823\n",
      "Epoch 046 | Val Acc: 0.8895\n",
      "Epoch 047 | Val Acc: 0.8750\n",
      "Epoch 048 | Val Acc: 0.8573\n",
      "Epoch 049 | Val Acc: 0.8516\n",
      "Epoch 050 | Val Acc: 0.8234\n",
      "Epoch 051 | Val Acc: 0.8444\n",
      "Epoch 052 | Val Acc: 0.8702\n",
      "Epoch 053 | Val Acc: 0.8847\n",
      "Epoch 054 | Val Acc: 0.8871\n",
      "Early stopping\n",
      "Split Accuracy: 0.8903\n",
      "\n",
      "----- SPLIT 10 -----\n",
      "Epoch 001 | Val Acc: 0.0274\n",
      "Epoch 002 | Val Acc: 0.1718\n",
      "Epoch 003 | Val Acc: 0.4169\n",
      "Epoch 004 | Val Acc: 0.5363\n",
      "Epoch 005 | Val Acc: 0.6218\n",
      "Epoch 006 | Val Acc: 0.6919\n",
      "Epoch 007 | Val Acc: 0.6895\n",
      "Epoch 008 | Val Acc: 0.6992\n",
      "Epoch 009 | Val Acc: 0.7363\n",
      "Epoch 010 | Val Acc: 0.7621\n",
      "Epoch 011 | Val Acc: 0.7750\n",
      "Epoch 012 | Val Acc: 0.7823\n",
      "Epoch 013 | Val Acc: 0.8129\n",
      "Epoch 014 | Val Acc: 0.7879\n",
      "Epoch 015 | Val Acc: 0.7661\n",
      "Epoch 016 | Val Acc: 0.8266\n",
      "Epoch 017 | Val Acc: 0.8250\n",
      "Epoch 018 | Val Acc: 0.8129\n",
      "Epoch 019 | Val Acc: 0.8258\n",
      "Epoch 020 | Val Acc: 0.8242\n",
      "Epoch 021 | Val Acc: 0.8121\n",
      "Epoch 022 | Val Acc: 0.8331\n",
      "Epoch 023 | Val Acc: 0.8226\n",
      "Epoch 024 | Val Acc: 0.8282\n",
      "Epoch 025 | Val Acc: 0.8347\n",
      "Epoch 026 | Val Acc: 0.8516\n",
      "Epoch 027 | Val Acc: 0.8395\n",
      "Epoch 028 | Val Acc: 0.8435\n",
      "Epoch 029 | Val Acc: 0.8323\n",
      "Epoch 030 | Val Acc: 0.8379\n",
      "Epoch 031 | Val Acc: 0.8234\n",
      "Epoch 032 | Val Acc: 0.8371\n",
      "Epoch 033 | Val Acc: 0.8419\n",
      "Epoch 034 | Val Acc: 0.8282\n",
      "Epoch 035 | Val Acc: 0.8581\n",
      "Epoch 036 | Val Acc: 0.8661\n",
      "Epoch 037 | Val Acc: 0.8581\n",
      "Epoch 038 | Val Acc: 0.8621\n",
      "Epoch 039 | Val Acc: 0.8266\n",
      "Epoch 040 | Val Acc: 0.8339\n",
      "Epoch 041 | Val Acc: 0.8282\n",
      "Epoch 042 | Val Acc: 0.8484\n",
      "Epoch 043 | Val Acc: 0.8565\n",
      "Epoch 044 | Val Acc: 0.8629\n",
      "Epoch 045 | Val Acc: 0.8621\n",
      "Epoch 046 | Val Acc: 0.8589\n",
      "Early stopping\n",
      "Split Accuracy: 0.8661\n",
      "\n",
      ">>> Average Accuracy (Triplet_L2): 0.8643\n",
      "\n",
      "==============================\n",
      " Encoder Loss: Triplet_Cosine\n",
      "==============================\n",
      "\n",
      "----- SPLIT 1 -----\n",
      "Epoch 001 | Val Acc: 0.0468\n",
      "Epoch 002 | Val Acc: 0.2298\n",
      "Epoch 003 | Val Acc: 0.4065\n",
      "Epoch 004 | Val Acc: 0.4935\n",
      "Epoch 005 | Val Acc: 0.5419\n",
      "Epoch 006 | Val Acc: 0.5871\n",
      "Epoch 007 | Val Acc: 0.5984\n",
      "Epoch 008 | Val Acc: 0.6040\n",
      "Epoch 009 | Val Acc: 0.6556\n",
      "Epoch 010 | Val Acc: 0.6589\n",
      "Epoch 011 | Val Acc: 0.7105\n",
      "Epoch 012 | Val Acc: 0.6823\n",
      "Epoch 013 | Val Acc: 0.6976\n",
      "Epoch 014 | Val Acc: 0.6984\n",
      "Epoch 015 | Val Acc: 0.6976\n",
      "Epoch 016 | Val Acc: 0.6839\n",
      "Epoch 017 | Val Acc: 0.7347\n",
      "Epoch 018 | Val Acc: 0.7145\n",
      "Epoch 019 | Val Acc: 0.7016\n",
      "Epoch 020 | Val Acc: 0.7169\n",
      "Epoch 021 | Val Acc: 0.7323\n",
      "Epoch 022 | Val Acc: 0.7347\n",
      "Epoch 023 | Val Acc: 0.7339\n",
      "Epoch 024 | Val Acc: 0.7548\n",
      "Epoch 025 | Val Acc: 0.7363\n",
      "Epoch 026 | Val Acc: 0.7460\n",
      "Epoch 027 | Val Acc: 0.7073\n",
      "Epoch 028 | Val Acc: 0.7250\n",
      "Epoch 029 | Val Acc: 0.7427\n",
      "Epoch 030 | Val Acc: 0.7242\n",
      "Epoch 031 | Val Acc: 0.7419\n",
      "Epoch 032 | Val Acc: 0.7460\n",
      "Epoch 033 | Val Acc: 0.7508\n",
      "Epoch 034 | Val Acc: 0.7484\n",
      "Early stopping\n",
      "Split Accuracy: 0.7548\n",
      "\n",
      "----- SPLIT 2 -----\n",
      "Epoch 001 | Val Acc: 0.0556\n",
      "Epoch 002 | Val Acc: 0.2250\n",
      "Epoch 003 | Val Acc: 0.3855\n",
      "Epoch 004 | Val Acc: 0.4935\n",
      "Epoch 005 | Val Acc: 0.5944\n",
      "Epoch 006 | Val Acc: 0.6540\n",
      "Epoch 007 | Val Acc: 0.7008\n",
      "Epoch 008 | Val Acc: 0.7250\n",
      "Epoch 009 | Val Acc: 0.7097\n",
      "Epoch 010 | Val Acc: 0.7677\n",
      "Epoch 011 | Val Acc: 0.7887\n",
      "Epoch 012 | Val Acc: 0.7532\n",
      "Epoch 013 | Val Acc: 0.7710\n",
      "Epoch 014 | Val Acc: 0.8065\n",
      "Epoch 015 | Val Acc: 0.8008\n",
      "Epoch 016 | Val Acc: 0.7895\n",
      "Epoch 017 | Val Acc: 0.8000\n",
      "Epoch 018 | Val Acc: 0.8129\n",
      "Epoch 019 | Val Acc: 0.8040\n",
      "Epoch 020 | Val Acc: 0.7952\n",
      "Epoch 021 | Val Acc: 0.8339\n",
      "Epoch 022 | Val Acc: 0.8153\n",
      "Epoch 023 | Val Acc: 0.8065\n",
      "Epoch 024 | Val Acc: 0.8242\n",
      "Epoch 025 | Val Acc: 0.8290\n",
      "Epoch 026 | Val Acc: 0.8347\n",
      "Epoch 027 | Val Acc: 0.8040\n",
      "Epoch 028 | Val Acc: 0.8105\n",
      "Epoch 029 | Val Acc: 0.8234\n",
      "Epoch 030 | Val Acc: 0.8137\n",
      "Epoch 031 | Val Acc: 0.8065\n",
      "Epoch 032 | Val Acc: 0.8210\n",
      "Epoch 033 | Val Acc: 0.8282\n",
      "Epoch 034 | Val Acc: 0.8266\n",
      "Epoch 035 | Val Acc: 0.8492\n",
      "Epoch 036 | Val Acc: 0.8395\n",
      "Epoch 037 | Val Acc: 0.8363\n",
      "Epoch 038 | Val Acc: 0.8395\n",
      "Epoch 039 | Val Acc: 0.8194\n",
      "Epoch 040 | Val Acc: 0.8306\n",
      "Epoch 041 | Val Acc: 0.8137\n",
      "Epoch 042 | Val Acc: 0.8177\n",
      "Epoch 043 | Val Acc: 0.8210\n",
      "Epoch 044 | Val Acc: 0.8266\n",
      "Epoch 045 | Val Acc: 0.8573\n",
      "Epoch 046 | Val Acc: 0.8484\n",
      "Epoch 047 | Val Acc: 0.8661\n",
      "Epoch 048 | Val Acc: 0.8669\n",
      "Epoch 049 | Val Acc: 0.8669\n",
      "Epoch 050 | Val Acc: 0.8605\n",
      "Epoch 051 | Val Acc: 0.8597\n",
      "Epoch 052 | Val Acc: 0.8524\n",
      "Epoch 053 | Val Acc: 0.8411\n",
      "Epoch 054 | Val Acc: 0.7460\n",
      "Epoch 055 | Val Acc: 0.8323\n",
      "Epoch 056 | Val Acc: 0.8185\n",
      "Epoch 057 | Val Acc: 0.8492\n",
      "Epoch 058 | Val Acc: 0.8419\n",
      "Early stopping\n",
      "Split Accuracy: 0.8669\n",
      "\n",
      "----- SPLIT 3 -----\n",
      "Epoch 001 | Val Acc: 0.0355\n",
      "Epoch 002 | Val Acc: 0.2718\n",
      "Epoch 003 | Val Acc: 0.4484\n",
      "Epoch 004 | Val Acc: 0.5742\n",
      "Epoch 005 | Val Acc: 0.6839\n",
      "Epoch 006 | Val Acc: 0.6871\n",
      "Epoch 007 | Val Acc: 0.7339\n",
      "Epoch 008 | Val Acc: 0.7597\n",
      "Epoch 009 | Val Acc: 0.7911\n",
      "Epoch 010 | Val Acc: 0.7839\n",
      "Epoch 011 | Val Acc: 0.7613\n",
      "Epoch 012 | Val Acc: 0.7798\n",
      "Epoch 013 | Val Acc: 0.7734\n",
      "Epoch 014 | Val Acc: 0.8097\n",
      "Epoch 015 | Val Acc: 0.7992\n",
      "Epoch 016 | Val Acc: 0.7992\n",
      "Epoch 017 | Val Acc: 0.8161\n",
      "Epoch 018 | Val Acc: 0.7944\n",
      "Epoch 019 | Val Acc: 0.8274\n",
      "Epoch 020 | Val Acc: 0.8177\n",
      "Epoch 021 | Val Acc: 0.8242\n",
      "Epoch 022 | Val Acc: 0.8387\n",
      "Epoch 023 | Val Acc: 0.8266\n",
      "Epoch 024 | Val Acc: 0.8387\n",
      "Epoch 025 | Val Acc: 0.8242\n",
      "Epoch 026 | Val Acc: 0.8323\n",
      "Epoch 027 | Val Acc: 0.8419\n",
      "Epoch 028 | Val Acc: 0.8379\n",
      "Epoch 029 | Val Acc: 0.8040\n",
      "Epoch 030 | Val Acc: 0.8242\n",
      "Epoch 031 | Val Acc: 0.8306\n",
      "Epoch 032 | Val Acc: 0.8137\n",
      "Epoch 033 | Val Acc: 0.8194\n",
      "Epoch 034 | Val Acc: 0.8339\n",
      "Epoch 035 | Val Acc: 0.8306\n",
      "Epoch 036 | Val Acc: 0.8565\n",
      "Epoch 037 | Val Acc: 0.8379\n",
      "Epoch 038 | Val Acc: 0.8137\n",
      "Epoch 039 | Val Acc: 0.8331\n",
      "Epoch 040 | Val Acc: 0.8282\n",
      "Epoch 041 | Val Acc: 0.8460\n",
      "Epoch 042 | Val Acc: 0.8403\n",
      "Epoch 043 | Val Acc: 0.8452\n",
      "Epoch 044 | Val Acc: 0.8605\n",
      "Epoch 045 | Val Acc: 0.8629\n",
      "Epoch 046 | Val Acc: 0.8597\n",
      "Epoch 047 | Val Acc: 0.8597\n",
      "Epoch 048 | Val Acc: 0.8669\n",
      "Epoch 049 | Val Acc: 0.8661\n",
      "Epoch 050 | Val Acc: 0.8653\n",
      "Epoch 051 | Val Acc: 0.8629\n",
      "Epoch 052 | Val Acc: 0.8685\n",
      "Epoch 053 | Val Acc: 0.8500\n",
      "Epoch 054 | Val Acc: 0.7597\n",
      "Epoch 055 | Val Acc: 0.7911\n",
      "Epoch 056 | Val Acc: 0.8290\n",
      "Epoch 057 | Val Acc: 0.8403\n",
      "Epoch 058 | Val Acc: 0.8468\n",
      "Epoch 059 | Val Acc: 0.8532\n",
      "Epoch 060 | Val Acc: 0.8548\n",
      "Epoch 061 | Val Acc: 0.8685\n",
      "Epoch 062 | Val Acc: 0.8669\n",
      "Early stopping\n",
      "Split Accuracy: 0.8685\n",
      "\n",
      "----- SPLIT 4 -----\n",
      "Epoch 001 | Val Acc: 0.0331\n",
      "Epoch 002 | Val Acc: 0.2008\n",
      "Epoch 003 | Val Acc: 0.4484\n",
      "Epoch 004 | Val Acc: 0.5718\n",
      "Epoch 005 | Val Acc: 0.6113\n",
      "Epoch 006 | Val Acc: 0.6798\n",
      "Epoch 007 | Val Acc: 0.7105\n",
      "Epoch 008 | Val Acc: 0.7153\n",
      "Epoch 009 | Val Acc: 0.7589\n",
      "Epoch 010 | Val Acc: 0.7895\n",
      "Epoch 011 | Val Acc: 0.8081\n",
      "Epoch 012 | Val Acc: 0.8081\n",
      "Epoch 013 | Val Acc: 0.8177\n",
      "Epoch 014 | Val Acc: 0.8452\n",
      "Epoch 015 | Val Acc: 0.8540\n",
      "Epoch 016 | Val Acc: 0.8274\n",
      "Epoch 017 | Val Acc: 0.8492\n",
      "Epoch 018 | Val Acc: 0.8315\n",
      "Epoch 019 | Val Acc: 0.8444\n",
      "Epoch 020 | Val Acc: 0.8250\n",
      "Epoch 021 | Val Acc: 0.8395\n",
      "Epoch 022 | Val Acc: 0.8323\n",
      "Epoch 023 | Val Acc: 0.8548\n",
      "Epoch 024 | Val Acc: 0.8339\n",
      "Epoch 025 | Val Acc: 0.8565\n",
      "Epoch 026 | Val Acc: 0.8540\n",
      "Epoch 027 | Val Acc: 0.8419\n",
      "Epoch 028 | Val Acc: 0.8161\n",
      "Epoch 029 | Val Acc: 0.8435\n",
      "Epoch 030 | Val Acc: 0.8339\n",
      "Epoch 031 | Val Acc: 0.8565\n",
      "Epoch 032 | Val Acc: 0.8548\n",
      "Epoch 033 | Val Acc: 0.8750\n",
      "Epoch 034 | Val Acc: 0.8556\n",
      "Epoch 035 | Val Acc: 0.8419\n",
      "Epoch 036 | Val Acc: 0.8452\n",
      "Epoch 037 | Val Acc: 0.8444\n",
      "Epoch 038 | Val Acc: 0.8637\n",
      "Epoch 039 | Val Acc: 0.8806\n",
      "Epoch 040 | Val Acc: 0.8734\n",
      "Epoch 041 | Val Acc: 0.8766\n",
      "Epoch 042 | Val Acc: 0.8742\n",
      "Epoch 043 | Val Acc: 0.8766\n",
      "Epoch 044 | Val Acc: 0.8855\n",
      "Epoch 045 | Val Acc: 0.8306\n",
      "Epoch 046 | Val Acc: 0.8081\n",
      "Epoch 047 | Val Acc: 0.8532\n",
      "Epoch 048 | Val Acc: 0.8653\n",
      "Epoch 049 | Val Acc: 0.8637\n",
      "Epoch 050 | Val Acc: 0.8726\n",
      "Epoch 051 | Val Acc: 0.8742\n",
      "Epoch 052 | Val Acc: 0.8702\n",
      "Epoch 053 | Val Acc: 0.8726\n",
      "Epoch 054 | Val Acc: 0.8782\n",
      "Early stopping\n",
      "Split Accuracy: 0.8855\n",
      "\n",
      "----- SPLIT 5 -----\n",
      "Epoch 001 | Val Acc: 0.0565\n",
      "Epoch 002 | Val Acc: 0.3016\n",
      "Epoch 003 | Val Acc: 0.4637\n",
      "Epoch 004 | Val Acc: 0.5911\n",
      "Epoch 005 | Val Acc: 0.6669\n",
      "Epoch 006 | Val Acc: 0.6992\n",
      "Epoch 007 | Val Acc: 0.7540\n",
      "Epoch 008 | Val Acc: 0.7774\n",
      "Epoch 009 | Val Acc: 0.7766\n",
      "Epoch 010 | Val Acc: 0.7871\n",
      "Epoch 011 | Val Acc: 0.8137\n",
      "Epoch 012 | Val Acc: 0.8145\n",
      "Epoch 013 | Val Acc: 0.8347\n",
      "Epoch 014 | Val Acc: 0.8081\n",
      "Epoch 015 | Val Acc: 0.8484\n",
      "Epoch 016 | Val Acc: 0.8411\n",
      "Epoch 017 | Val Acc: 0.8315\n",
      "Epoch 018 | Val Acc: 0.8444\n",
      "Epoch 019 | Val Acc: 0.8427\n",
      "Epoch 020 | Val Acc: 0.8548\n",
      "Epoch 021 | Val Acc: 0.8452\n",
      "Epoch 022 | Val Acc: 0.8121\n",
      "Epoch 023 | Val Acc: 0.8403\n",
      "Epoch 024 | Val Acc: 0.8266\n",
      "Epoch 025 | Val Acc: 0.8565\n",
      "Epoch 026 | Val Acc: 0.8508\n",
      "Epoch 027 | Val Acc: 0.8621\n",
      "Epoch 028 | Val Acc: 0.8702\n",
      "Epoch 029 | Val Acc: 0.8597\n",
      "Epoch 030 | Val Acc: 0.8750\n",
      "Epoch 031 | Val Acc: 0.8710\n",
      "Epoch 032 | Val Acc: 0.8613\n",
      "Epoch 033 | Val Acc: 0.8194\n",
      "Epoch 034 | Val Acc: 0.8435\n",
      "Epoch 035 | Val Acc: 0.8395\n",
      "Epoch 036 | Val Acc: 0.8508\n",
      "Epoch 037 | Val Acc: 0.8573\n",
      "Epoch 038 | Val Acc: 0.8669\n",
      "Epoch 039 | Val Acc: 0.8774\n",
      "Epoch 040 | Val Acc: 0.8831\n",
      "Epoch 041 | Val Acc: 0.8847\n",
      "Epoch 042 | Val Acc: 0.8855\n",
      "Epoch 043 | Val Acc: 0.8927\n",
      "Epoch 044 | Val Acc: 0.8806\n",
      "Epoch 045 | Val Acc: 0.8903\n",
      "Epoch 046 | Val Acc: 0.8887\n",
      "Epoch 047 | Val Acc: 0.8355\n",
      "Epoch 048 | Val Acc: 0.8637\n",
      "Epoch 049 | Val Acc: 0.8573\n",
      "Epoch 050 | Val Acc: 0.8677\n",
      "Epoch 051 | Val Acc: 0.8831\n",
      "Epoch 052 | Val Acc: 0.8887\n",
      "Epoch 053 | Val Acc: 0.8984\n",
      "Epoch 054 | Val Acc: 0.8935\n",
      "Epoch 055 | Val Acc: 0.8952\n",
      "Epoch 056 | Val Acc: 0.8944\n",
      "Epoch 057 | Val Acc: 0.8944\n",
      "Epoch 058 | Val Acc: 0.8992\n",
      "Epoch 059 | Val Acc: 0.8927\n",
      "Epoch 060 | Val Acc: 0.8919\n",
      "Epoch 061 | Val Acc: 0.8790\n",
      "Epoch 062 | Val Acc: 0.8113\n",
      "Epoch 063 | Val Acc: 0.8258\n",
      "Epoch 064 | Val Acc: 0.8516\n",
      "Epoch 065 | Val Acc: 0.8734\n",
      "Epoch 066 | Val Acc: 0.8831\n",
      "Epoch 067 | Val Acc: 0.8879\n",
      "Epoch 068 | Val Acc: 0.9016\n",
      "Epoch 069 | Val Acc: 0.9008\n",
      "Epoch 070 | Val Acc: 0.8976\n",
      "Epoch 071 | Val Acc: 0.8944\n",
      "Epoch 072 | Val Acc: 0.8984\n",
      "Epoch 073 | Val Acc: 0.8968\n",
      "Epoch 074 | Val Acc: 0.8944\n",
      "Epoch 075 | Val Acc: 0.8976\n",
      "Epoch 076 | Val Acc: 0.9000\n",
      "Epoch 077 | Val Acc: 0.8960\n",
      "Epoch 078 | Val Acc: 0.8895\n",
      "Early stopping\n",
      "Split Accuracy: 0.9016\n",
      "\n",
      "----- SPLIT 6 -----\n",
      "Epoch 001 | Val Acc: 0.0194\n",
      "Epoch 002 | Val Acc: 0.0919\n",
      "Epoch 003 | Val Acc: 0.3726\n",
      "Epoch 004 | Val Acc: 0.5266\n",
      "Epoch 005 | Val Acc: 0.6484\n",
      "Epoch 006 | Val Acc: 0.6766\n",
      "Epoch 007 | Val Acc: 0.7347\n",
      "Epoch 008 | Val Acc: 0.7411\n",
      "Epoch 009 | Val Acc: 0.7855\n",
      "Epoch 010 | Val Acc: 0.7605\n",
      "Epoch 011 | Val Acc: 0.7984\n",
      "Epoch 012 | Val Acc: 0.7879\n",
      "Epoch 013 | Val Acc: 0.8210\n",
      "Epoch 014 | Val Acc: 0.8137\n",
      "Epoch 015 | Val Acc: 0.8282\n",
      "Epoch 016 | Val Acc: 0.8403\n",
      "Epoch 017 | Val Acc: 0.8121\n",
      "Epoch 018 | Val Acc: 0.8379\n",
      "Epoch 019 | Val Acc: 0.8129\n",
      "Epoch 020 | Val Acc: 0.8315\n",
      "Epoch 021 | Val Acc: 0.8468\n",
      "Epoch 022 | Val Acc: 0.8427\n",
      "Epoch 023 | Val Acc: 0.8444\n",
      "Epoch 024 | Val Acc: 0.8524\n",
      "Epoch 025 | Val Acc: 0.8452\n",
      "Epoch 026 | Val Acc: 0.8387\n",
      "Epoch 027 | Val Acc: 0.8371\n",
      "Epoch 028 | Val Acc: 0.8532\n",
      "Epoch 029 | Val Acc: 0.8637\n",
      "Epoch 030 | Val Acc: 0.8839\n",
      "Epoch 031 | Val Acc: 0.8702\n",
      "Epoch 032 | Val Acc: 0.8476\n",
      "Epoch 033 | Val Acc: 0.8177\n",
      "Epoch 034 | Val Acc: 0.8395\n",
      "Epoch 035 | Val Acc: 0.8516\n",
      "Epoch 036 | Val Acc: 0.8492\n",
      "Epoch 037 | Val Acc: 0.8492\n",
      "Epoch 038 | Val Acc: 0.8548\n",
      "Epoch 039 | Val Acc: 0.8798\n",
      "Epoch 040 | Val Acc: 0.8798\n",
      "Early stopping\n",
      "Split Accuracy: 0.8839\n",
      "\n",
      "----- SPLIT 7 -----\n",
      "Epoch 001 | Val Acc: 0.0266\n",
      "Epoch 002 | Val Acc: 0.1226\n",
      "Epoch 003 | Val Acc: 0.3952\n",
      "Epoch 004 | Val Acc: 0.5387\n",
      "Epoch 005 | Val Acc: 0.6444\n",
      "Epoch 006 | Val Acc: 0.6734\n",
      "Epoch 007 | Val Acc: 0.7024\n",
      "Epoch 008 | Val Acc: 0.7694\n",
      "Epoch 009 | Val Acc: 0.7798\n",
      "Epoch 010 | Val Acc: 0.7823\n",
      "Epoch 011 | Val Acc: 0.7758\n",
      "Epoch 012 | Val Acc: 0.8105\n",
      "Epoch 013 | Val Acc: 0.8266\n",
      "Epoch 014 | Val Acc: 0.8153\n",
      "Epoch 015 | Val Acc: 0.8315\n",
      "Epoch 016 | Val Acc: 0.8331\n",
      "Epoch 017 | Val Acc: 0.8315\n",
      "Epoch 018 | Val Acc: 0.8169\n",
      "Epoch 019 | Val Acc: 0.8581\n",
      "Epoch 020 | Val Acc: 0.8379\n",
      "Epoch 021 | Val Acc: 0.8444\n",
      "Epoch 022 | Val Acc: 0.8371\n",
      "Epoch 023 | Val Acc: 0.8339\n",
      "Epoch 024 | Val Acc: 0.8645\n",
      "Epoch 025 | Val Acc: 0.8427\n",
      "Epoch 026 | Val Acc: 0.8460\n",
      "Epoch 027 | Val Acc: 0.8621\n",
      "Epoch 028 | Val Acc: 0.8581\n",
      "Epoch 029 | Val Acc: 0.8460\n",
      "Epoch 030 | Val Acc: 0.8613\n",
      "Epoch 031 | Val Acc: 0.8403\n",
      "Epoch 032 | Val Acc: 0.8565\n",
      "Epoch 033 | Val Acc: 0.8419\n",
      "Epoch 034 | Val Acc: 0.8613\n",
      "Early stopping\n",
      "Split Accuracy: 0.8645\n",
      "\n",
      "----- SPLIT 8 -----\n",
      "Epoch 001 | Val Acc: 0.0444\n",
      "Epoch 002 | Val Acc: 0.2871\n",
      "Epoch 003 | Val Acc: 0.4500\n",
      "Epoch 004 | Val Acc: 0.5685\n",
      "Epoch 005 | Val Acc: 0.6153\n",
      "Epoch 006 | Val Acc: 0.6895\n",
      "Epoch 007 | Val Acc: 0.6823\n",
      "Epoch 008 | Val Acc: 0.7169\n",
      "Epoch 009 | Val Acc: 0.7355\n",
      "Epoch 010 | Val Acc: 0.7629\n",
      "Epoch 011 | Val Acc: 0.7823\n",
      "Epoch 012 | Val Acc: 0.7694\n",
      "Epoch 013 | Val Acc: 0.7855\n",
      "Epoch 014 | Val Acc: 0.7750\n",
      "Epoch 015 | Val Acc: 0.7895\n",
      "Epoch 016 | Val Acc: 0.8073\n",
      "Epoch 017 | Val Acc: 0.7847\n",
      "Epoch 018 | Val Acc: 0.8161\n",
      "Epoch 019 | Val Acc: 0.8210\n",
      "Epoch 020 | Val Acc: 0.8210\n",
      "Epoch 021 | Val Acc: 0.8177\n",
      "Epoch 022 | Val Acc: 0.8073\n",
      "Epoch 023 | Val Acc: 0.8218\n",
      "Epoch 024 | Val Acc: 0.8137\n",
      "Epoch 025 | Val Acc: 0.8105\n",
      "Epoch 026 | Val Acc: 0.8177\n",
      "Epoch 027 | Val Acc: 0.8226\n",
      "Epoch 028 | Val Acc: 0.8161\n",
      "Epoch 029 | Val Acc: 0.8194\n",
      "Epoch 030 | Val Acc: 0.8331\n",
      "Epoch 031 | Val Acc: 0.8266\n",
      "Epoch 032 | Val Acc: 0.8145\n",
      "Epoch 033 | Val Acc: 0.7976\n",
      "Epoch 034 | Val Acc: 0.8121\n",
      "Epoch 035 | Val Acc: 0.8040\n",
      "Epoch 036 | Val Acc: 0.8355\n",
      "Epoch 037 | Val Acc: 0.8371\n",
      "Epoch 038 | Val Acc: 0.8403\n",
      "Epoch 039 | Val Acc: 0.8468\n",
      "Epoch 040 | Val Acc: 0.8492\n",
      "Epoch 041 | Val Acc: 0.8419\n",
      "Epoch 042 | Val Acc: 0.8492\n",
      "Epoch 043 | Val Acc: 0.8347\n",
      "Epoch 044 | Val Acc: 0.8290\n",
      "Epoch 045 | Val Acc: 0.7992\n",
      "Epoch 046 | Val Acc: 0.8105\n",
      "Epoch 047 | Val Acc: 0.8137\n",
      "Epoch 048 | Val Acc: 0.8331\n",
      "Epoch 049 | Val Acc: 0.8387\n",
      "Epoch 050 | Val Acc: 0.8484\n",
      "Early stopping\n",
      "Split Accuracy: 0.8492\n",
      "\n",
      "----- SPLIT 9 -----\n",
      "Epoch 001 | Val Acc: 0.0226\n",
      "Epoch 002 | Val Acc: 0.1919\n",
      "Epoch 003 | Val Acc: 0.4169\n",
      "Epoch 004 | Val Acc: 0.5581\n",
      "Epoch 005 | Val Acc: 0.6218\n",
      "Epoch 006 | Val Acc: 0.6637\n",
      "Epoch 007 | Val Acc: 0.6806\n",
      "Epoch 008 | Val Acc: 0.7282\n",
      "Epoch 009 | Val Acc: 0.7556\n",
      "Epoch 010 | Val Acc: 0.7758\n",
      "Epoch 011 | Val Acc: 0.7694\n",
      "Epoch 012 | Val Acc: 0.7815\n",
      "Epoch 013 | Val Acc: 0.8169\n",
      "Epoch 014 | Val Acc: 0.8016\n",
      "Epoch 015 | Val Acc: 0.8113\n",
      "Epoch 016 | Val Acc: 0.8008\n",
      "Epoch 017 | Val Acc: 0.7871\n",
      "Epoch 018 | Val Acc: 0.8145\n",
      "Epoch 019 | Val Acc: 0.8185\n",
      "Epoch 020 | Val Acc: 0.8484\n",
      "Epoch 021 | Val Acc: 0.8323\n",
      "Epoch 022 | Val Acc: 0.8250\n",
      "Epoch 023 | Val Acc: 0.8056\n",
      "Epoch 024 | Val Acc: 0.8403\n",
      "Epoch 025 | Val Acc: 0.8355\n",
      "Epoch 026 | Val Acc: 0.8508\n",
      "Epoch 027 | Val Acc: 0.8395\n",
      "Epoch 028 | Val Acc: 0.8323\n",
      "Epoch 029 | Val Acc: 0.8435\n",
      "Epoch 030 | Val Acc: 0.8403\n",
      "Epoch 031 | Val Acc: 0.8347\n",
      "Epoch 032 | Val Acc: 0.8524\n",
      "Epoch 033 | Val Acc: 0.8605\n",
      "Epoch 034 | Val Acc: 0.8613\n",
      "Epoch 035 | Val Acc: 0.8629\n",
      "Epoch 036 | Val Acc: 0.8371\n",
      "Epoch 037 | Val Acc: 0.8129\n",
      "Epoch 038 | Val Acc: 0.8210\n",
      "Epoch 039 | Val Acc: 0.8363\n",
      "Epoch 040 | Val Acc: 0.8524\n",
      "Epoch 041 | Val Acc: 0.8427\n",
      "Epoch 042 | Val Acc: 0.8718\n",
      "Epoch 043 | Val Acc: 0.8726\n",
      "Epoch 044 | Val Acc: 0.8766\n",
      "Epoch 045 | Val Acc: 0.8702\n",
      "Epoch 046 | Val Acc: 0.8573\n",
      "Epoch 047 | Val Acc: 0.8387\n",
      "Epoch 048 | Val Acc: 0.7911\n",
      "Epoch 049 | Val Acc: 0.8315\n",
      "Epoch 050 | Val Acc: 0.8347\n",
      "Epoch 051 | Val Acc: 0.8661\n",
      "Epoch 052 | Val Acc: 0.8653\n",
      "Epoch 053 | Val Acc: 0.8831\n",
      "Epoch 054 | Val Acc: 0.8565\n",
      "Epoch 055 | Val Acc: 0.8742\n",
      "Epoch 056 | Val Acc: 0.8919\n",
      "Epoch 057 | Val Acc: 0.8750\n",
      "Epoch 058 | Val Acc: 0.8806\n",
      "Epoch 059 | Val Acc: 0.8855\n",
      "Epoch 060 | Val Acc: 0.8782\n",
      "Epoch 061 | Val Acc: 0.8798\n",
      "Epoch 062 | Val Acc: 0.8798\n",
      "Epoch 063 | Val Acc: 0.8806\n",
      "Epoch 064 | Val Acc: 0.8879\n",
      "Epoch 065 | Val Acc: 0.8758\n",
      "Epoch 066 | Val Acc: 0.8500\n",
      "Early stopping\n",
      "Split Accuracy: 0.8919\n",
      "\n",
      "----- SPLIT 10 -----\n",
      "Epoch 001 | Val Acc: 0.0444\n",
      "Epoch 002 | Val Acc: 0.2556\n",
      "Epoch 003 | Val Acc: 0.4669\n",
      "Epoch 004 | Val Acc: 0.5427\n",
      "Epoch 005 | Val Acc: 0.5790\n",
      "Epoch 006 | Val Acc: 0.6782\n",
      "Epoch 007 | Val Acc: 0.7081\n",
      "Epoch 008 | Val Acc: 0.7250\n",
      "Epoch 009 | Val Acc: 0.7452\n",
      "Epoch 010 | Val Acc: 0.7476\n",
      "Epoch 011 | Val Acc: 0.7605\n",
      "Epoch 012 | Val Acc: 0.7879\n",
      "Epoch 013 | Val Acc: 0.7710\n",
      "Epoch 014 | Val Acc: 0.7427\n",
      "Epoch 015 | Val Acc: 0.8024\n",
      "Epoch 016 | Val Acc: 0.8048\n",
      "Epoch 017 | Val Acc: 0.8000\n",
      "Epoch 018 | Val Acc: 0.8129\n",
      "Epoch 019 | Val Acc: 0.8242\n",
      "Epoch 020 | Val Acc: 0.8105\n",
      "Epoch 021 | Val Acc: 0.7895\n",
      "Epoch 022 | Val Acc: 0.8073\n",
      "Epoch 023 | Val Acc: 0.7968\n",
      "Epoch 024 | Val Acc: 0.8177\n",
      "Epoch 025 | Val Acc: 0.8089\n",
      "Epoch 026 | Val Acc: 0.8113\n",
      "Epoch 027 | Val Acc: 0.8145\n",
      "Epoch 028 | Val Acc: 0.8137\n",
      "Epoch 029 | Val Acc: 0.8137\n",
      "Early stopping\n",
      "Split Accuracy: 0.8242\n",
      "\n",
      ">>> Average Accuracy (Triplet_Cosine): 0.8591\n",
      "\n",
      "==============================\n",
      " Encoder Loss: NTXent\n",
      "==============================\n",
      "\n",
      "----- SPLIT 1 -----\n",
      "Epoch 001 | Val Acc: 0.0218\n",
      "Epoch 002 | Val Acc: 0.0782\n",
      "Epoch 003 | Val Acc: 0.2508\n",
      "Epoch 004 | Val Acc: 0.4298\n",
      "Epoch 005 | Val Acc: 0.5202\n",
      "Epoch 006 | Val Acc: 0.5871\n",
      "Epoch 007 | Val Acc: 0.6040\n",
      "Epoch 008 | Val Acc: 0.6637\n",
      "Epoch 009 | Val Acc: 0.6685\n",
      "Epoch 010 | Val Acc: 0.6895\n",
      "Epoch 011 | Val Acc: 0.7161\n",
      "Epoch 012 | Val Acc: 0.7008\n",
      "Epoch 013 | Val Acc: 0.7403\n",
      "Epoch 014 | Val Acc: 0.7242\n",
      "Epoch 015 | Val Acc: 0.7315\n",
      "Epoch 016 | Val Acc: 0.7403\n",
      "Epoch 017 | Val Acc: 0.7379\n",
      "Epoch 018 | Val Acc: 0.7500\n",
      "Epoch 019 | Val Acc: 0.7694\n",
      "Epoch 020 | Val Acc: 0.7613\n",
      "Epoch 021 | Val Acc: 0.7532\n",
      "Epoch 022 | Val Acc: 0.7524\n",
      "Epoch 023 | Val Acc: 0.7742\n",
      "Epoch 024 | Val Acc: 0.7306\n",
      "Epoch 025 | Val Acc: 0.7766\n",
      "Epoch 026 | Val Acc: 0.7613\n",
      "Epoch 027 | Val Acc: 0.7613\n",
      "Epoch 028 | Val Acc: 0.7766\n",
      "Epoch 029 | Val Acc: 0.7726\n",
      "Epoch 030 | Val Acc: 0.7363\n",
      "Epoch 031 | Val Acc: 0.7758\n",
      "Epoch 032 | Val Acc: 0.7758\n",
      "Epoch 033 | Val Acc: 0.7798\n",
      "Epoch 034 | Val Acc: 0.7742\n",
      "Epoch 035 | Val Acc: 0.7532\n",
      "Epoch 036 | Val Acc: 0.7847\n",
      "Epoch 037 | Val Acc: 0.7621\n",
      "Epoch 038 | Val Acc: 0.7750\n",
      "Epoch 039 | Val Acc: 0.7669\n",
      "Epoch 040 | Val Acc: 0.7766\n",
      "Epoch 041 | Val Acc: 0.7694\n",
      "Epoch 042 | Val Acc: 0.7750\n",
      "Epoch 043 | Val Acc: 0.7556\n",
      "Epoch 044 | Val Acc: 0.7815\n",
      "Epoch 045 | Val Acc: 0.7750\n",
      "Epoch 046 | Val Acc: 0.7597\n",
      "Early stopping\n",
      "Split Accuracy: 0.7847\n",
      "\n",
      "----- SPLIT 2 -----\n",
      "Epoch 001 | Val Acc: 0.0258\n",
      "Epoch 002 | Val Acc: 0.1097\n",
      "Epoch 003 | Val Acc: 0.3242\n",
      "Epoch 004 | Val Acc: 0.5032\n",
      "Epoch 005 | Val Acc: 0.5774\n",
      "Epoch 006 | Val Acc: 0.6129\n",
      "Epoch 007 | Val Acc: 0.7129\n",
      "Epoch 008 | Val Acc: 0.7218\n",
      "Epoch 009 | Val Acc: 0.7387\n",
      "Epoch 010 | Val Acc: 0.7782\n",
      "Epoch 011 | Val Acc: 0.8056\n",
      "Epoch 012 | Val Acc: 0.7960\n",
      "Epoch 013 | Val Acc: 0.8210\n",
      "Epoch 014 | Val Acc: 0.8129\n",
      "Epoch 015 | Val Acc: 0.8242\n",
      "Epoch 016 | Val Acc: 0.8331\n",
      "Epoch 017 | Val Acc: 0.8363\n",
      "Epoch 018 | Val Acc: 0.8266\n",
      "Epoch 019 | Val Acc: 0.8379\n",
      "Epoch 020 | Val Acc: 0.8613\n",
      "Epoch 021 | Val Acc: 0.8306\n",
      "Epoch 022 | Val Acc: 0.8540\n",
      "Epoch 023 | Val Acc: 0.8621\n",
      "Epoch 024 | Val Acc: 0.8573\n",
      "Epoch 025 | Val Acc: 0.8540\n",
      "Epoch 026 | Val Acc: 0.8403\n",
      "Epoch 027 | Val Acc: 0.8589\n",
      "Epoch 028 | Val Acc: 0.8637\n",
      "Epoch 029 | Val Acc: 0.8476\n",
      "Epoch 030 | Val Acc: 0.8484\n",
      "Epoch 031 | Val Acc: 0.8516\n",
      "Epoch 032 | Val Acc: 0.8540\n",
      "Epoch 033 | Val Acc: 0.8589\n",
      "Epoch 034 | Val Acc: 0.8637\n",
      "Epoch 035 | Val Acc: 0.8734\n",
      "Epoch 036 | Val Acc: 0.8694\n",
      "Epoch 037 | Val Acc: 0.8661\n",
      "Epoch 038 | Val Acc: 0.8758\n",
      "Epoch 039 | Val Acc: 0.8661\n",
      "Epoch 040 | Val Acc: 0.8508\n",
      "Epoch 041 | Val Acc: 0.8565\n",
      "Epoch 042 | Val Acc: 0.8629\n",
      "Epoch 043 | Val Acc: 0.8532\n",
      "Epoch 044 | Val Acc: 0.8597\n",
      "Epoch 045 | Val Acc: 0.8556\n",
      "Epoch 046 | Val Acc: 0.8669\n",
      "Epoch 047 | Val Acc: 0.8702\n",
      "Epoch 048 | Val Acc: 0.8677\n",
      "Early stopping\n",
      "Split Accuracy: 0.8758\n",
      "\n",
      "----- SPLIT 3 -----\n",
      "Epoch 001 | Val Acc: 0.0298\n",
      "Epoch 002 | Val Acc: 0.0589\n",
      "Epoch 003 | Val Acc: 0.2250\n",
      "Epoch 004 | Val Acc: 0.4000\n",
      "Epoch 005 | Val Acc: 0.5137\n",
      "Epoch 006 | Val Acc: 0.6202\n",
      "Epoch 007 | Val Acc: 0.6573\n",
      "Epoch 008 | Val Acc: 0.6992\n",
      "Epoch 009 | Val Acc: 0.7492\n",
      "Epoch 010 | Val Acc: 0.7637\n",
      "Epoch 011 | Val Acc: 0.7613\n",
      "Epoch 012 | Val Acc: 0.7532\n",
      "Epoch 013 | Val Acc: 0.7903\n",
      "Epoch 014 | Val Acc: 0.8040\n",
      "Epoch 015 | Val Acc: 0.8016\n",
      "Epoch 016 | Val Acc: 0.8081\n",
      "Epoch 017 | Val Acc: 0.8121\n",
      "Epoch 018 | Val Acc: 0.8121\n",
      "Epoch 019 | Val Acc: 0.8218\n",
      "Epoch 020 | Val Acc: 0.8323\n",
      "Epoch 021 | Val Acc: 0.8339\n",
      "Epoch 022 | Val Acc: 0.8298\n",
      "Epoch 023 | Val Acc: 0.8395\n",
      "Epoch 024 | Val Acc: 0.8427\n",
      "Epoch 025 | Val Acc: 0.8323\n",
      "Epoch 026 | Val Acc: 0.8452\n",
      "Epoch 027 | Val Acc: 0.8556\n",
      "Epoch 028 | Val Acc: 0.8403\n",
      "Epoch 029 | Val Acc: 0.8524\n",
      "Epoch 030 | Val Acc: 0.8476\n",
      "Epoch 031 | Val Acc: 0.8581\n",
      "Epoch 032 | Val Acc: 0.8427\n",
      "Epoch 033 | Val Acc: 0.8484\n",
      "Epoch 034 | Val Acc: 0.8460\n",
      "Epoch 035 | Val Acc: 0.8355\n",
      "Epoch 036 | Val Acc: 0.8419\n",
      "Epoch 037 | Val Acc: 0.8597\n",
      "Epoch 038 | Val Acc: 0.8766\n",
      "Epoch 039 | Val Acc: 0.8613\n",
      "Epoch 040 | Val Acc: 0.8581\n",
      "Epoch 041 | Val Acc: 0.8556\n",
      "Epoch 042 | Val Acc: 0.8548\n",
      "Epoch 043 | Val Acc: 0.8702\n",
      "Epoch 044 | Val Acc: 0.8613\n",
      "Epoch 045 | Val Acc: 0.8710\n",
      "Epoch 046 | Val Acc: 0.8734\n",
      "Epoch 047 | Val Acc: 0.8685\n",
      "Epoch 048 | Val Acc: 0.8677\n",
      "Early stopping\n",
      "Split Accuracy: 0.8766\n",
      "\n",
      "----- SPLIT 4 -----\n",
      "Epoch 001 | Val Acc: 0.0177\n",
      "Epoch 002 | Val Acc: 0.0782\n",
      "Epoch 003 | Val Acc: 0.2855\n",
      "Epoch 004 | Val Acc: 0.5419\n",
      "Epoch 005 | Val Acc: 0.6056\n",
      "Epoch 006 | Val Acc: 0.6798\n",
      "Epoch 007 | Val Acc: 0.7347\n",
      "Epoch 008 | Val Acc: 0.7331\n",
      "Epoch 009 | Val Acc: 0.7758\n",
      "Epoch 010 | Val Acc: 0.7855\n",
      "Epoch 011 | Val Acc: 0.8081\n",
      "Epoch 012 | Val Acc: 0.8306\n",
      "Epoch 013 | Val Acc: 0.8089\n",
      "Epoch 014 | Val Acc: 0.8194\n",
      "Epoch 015 | Val Acc: 0.8323\n",
      "Epoch 016 | Val Acc: 0.8161\n",
      "Epoch 017 | Val Acc: 0.8468\n",
      "Epoch 018 | Val Acc: 0.8427\n",
      "Epoch 019 | Val Acc: 0.8524\n",
      "Epoch 020 | Val Acc: 0.8565\n",
      "Epoch 021 | Val Acc: 0.8411\n",
      "Epoch 022 | Val Acc: 0.8516\n",
      "Epoch 023 | Val Acc: 0.8597\n",
      "Epoch 024 | Val Acc: 0.8589\n",
      "Epoch 025 | Val Acc: 0.8766\n",
      "Epoch 026 | Val Acc: 0.8661\n",
      "Epoch 027 | Val Acc: 0.8613\n",
      "Epoch 028 | Val Acc: 0.8347\n",
      "Epoch 029 | Val Acc: 0.8540\n",
      "Epoch 030 | Val Acc: 0.8492\n",
      "Epoch 031 | Val Acc: 0.8685\n",
      "Epoch 032 | Val Acc: 0.8790\n",
      "Epoch 033 | Val Acc: 0.8702\n",
      "Epoch 034 | Val Acc: 0.8669\n",
      "Epoch 035 | Val Acc: 0.8589\n",
      "Epoch 036 | Val Acc: 0.8742\n",
      "Epoch 037 | Val Acc: 0.8782\n",
      "Epoch 038 | Val Acc: 0.8718\n",
      "Epoch 039 | Val Acc: 0.8637\n",
      "Epoch 040 | Val Acc: 0.8677\n",
      "Epoch 041 | Val Acc: 0.8653\n",
      "Epoch 042 | Val Acc: 0.8742\n",
      "Early stopping\n",
      "Split Accuracy: 0.8790\n",
      "\n",
      "----- SPLIT 5 -----\n",
      "Epoch 001 | Val Acc: 0.0210\n",
      "Epoch 002 | Val Acc: 0.0976\n",
      "Epoch 003 | Val Acc: 0.3129\n",
      "Epoch 004 | Val Acc: 0.5065\n",
      "Epoch 005 | Val Acc: 0.6419\n",
      "Epoch 006 | Val Acc: 0.6661\n",
      "Epoch 007 | Val Acc: 0.7177\n",
      "Epoch 008 | Val Acc: 0.7339\n",
      "Epoch 009 | Val Acc: 0.7661\n",
      "Epoch 010 | Val Acc: 0.7750\n",
      "Epoch 011 | Val Acc: 0.8089\n",
      "Epoch 012 | Val Acc: 0.7976\n",
      "Epoch 013 | Val Acc: 0.8194\n",
      "Epoch 014 | Val Acc: 0.8081\n",
      "Epoch 015 | Val Acc: 0.8444\n",
      "Epoch 016 | Val Acc: 0.8355\n",
      "Epoch 017 | Val Acc: 0.8323\n",
      "Epoch 018 | Val Acc: 0.8460\n",
      "Epoch 019 | Val Acc: 0.8573\n",
      "Epoch 020 | Val Acc: 0.8613\n",
      "Epoch 021 | Val Acc: 0.8669\n",
      "Epoch 022 | Val Acc: 0.8637\n",
      "Epoch 023 | Val Acc: 0.8540\n",
      "Epoch 024 | Val Acc: 0.8589\n",
      "Epoch 025 | Val Acc: 0.8685\n",
      "Epoch 026 | Val Acc: 0.8524\n",
      "Epoch 027 | Val Acc: 0.8540\n",
      "Epoch 028 | Val Acc: 0.8702\n",
      "Epoch 029 | Val Acc: 0.8734\n",
      "Epoch 030 | Val Acc: 0.8750\n",
      "Epoch 031 | Val Acc: 0.8685\n",
      "Epoch 032 | Val Acc: 0.8653\n",
      "Epoch 033 | Val Acc: 0.8863\n",
      "Epoch 034 | Val Acc: 0.8710\n",
      "Epoch 035 | Val Acc: 0.8718\n",
      "Epoch 036 | Val Acc: 0.8823\n",
      "Epoch 037 | Val Acc: 0.8782\n",
      "Epoch 038 | Val Acc: 0.8847\n",
      "Epoch 039 | Val Acc: 0.8847\n",
      "Epoch 040 | Val Acc: 0.8871\n",
      "Epoch 041 | Val Acc: 0.8823\n",
      "Epoch 042 | Val Acc: 0.8847\n",
      "Epoch 043 | Val Acc: 0.8815\n",
      "Epoch 044 | Val Acc: 0.8903\n",
      "Epoch 045 | Val Acc: 0.8887\n",
      "Epoch 046 | Val Acc: 0.9000\n",
      "Epoch 047 | Val Acc: 0.8806\n",
      "Epoch 048 | Val Acc: 0.8798\n",
      "Epoch 049 | Val Acc: 0.8718\n",
      "Epoch 050 | Val Acc: 0.8323\n",
      "Epoch 051 | Val Acc: 0.8879\n",
      "Epoch 052 | Val Acc: 0.8839\n",
      "Epoch 053 | Val Acc: 0.8895\n",
      "Epoch 054 | Val Acc: 0.8911\n",
      "Epoch 055 | Val Acc: 0.8944\n",
      "Epoch 056 | Val Acc: 0.8952\n",
      "Early stopping\n",
      "Split Accuracy: 0.9000\n",
      "\n",
      "----- SPLIT 6 -----\n",
      "Epoch 001 | Val Acc: 0.0137\n",
      "Epoch 002 | Val Acc: 0.0516\n",
      "Epoch 003 | Val Acc: 0.1645\n",
      "Epoch 004 | Val Acc: 0.3790\n",
      "Epoch 005 | Val Acc: 0.5153\n",
      "Epoch 006 | Val Acc: 0.5935\n",
      "Epoch 007 | Val Acc: 0.6782\n",
      "Epoch 008 | Val Acc: 0.6855\n",
      "Epoch 009 | Val Acc: 0.7331\n",
      "Epoch 010 | Val Acc: 0.7363\n",
      "Epoch 011 | Val Acc: 0.7790\n",
      "Epoch 012 | Val Acc: 0.7887\n",
      "Epoch 013 | Val Acc: 0.8048\n",
      "Epoch 014 | Val Acc: 0.8113\n",
      "Epoch 015 | Val Acc: 0.8073\n",
      "Epoch 016 | Val Acc: 0.8226\n",
      "Epoch 017 | Val Acc: 0.8339\n",
      "Epoch 018 | Val Acc: 0.8210\n",
      "Epoch 019 | Val Acc: 0.8250\n",
      "Epoch 020 | Val Acc: 0.8371\n",
      "Epoch 021 | Val Acc: 0.8331\n",
      "Epoch 022 | Val Acc: 0.8508\n",
      "Epoch 023 | Val Acc: 0.8371\n",
      "Epoch 024 | Val Acc: 0.8331\n",
      "Epoch 025 | Val Acc: 0.8395\n",
      "Epoch 026 | Val Acc: 0.8444\n",
      "Epoch 027 | Val Acc: 0.8589\n",
      "Epoch 028 | Val Acc: 0.8492\n",
      "Epoch 029 | Val Acc: 0.8476\n",
      "Epoch 030 | Val Acc: 0.8540\n",
      "Epoch 031 | Val Acc: 0.8548\n",
      "Epoch 032 | Val Acc: 0.8613\n",
      "Epoch 033 | Val Acc: 0.8460\n",
      "Epoch 034 | Val Acc: 0.8718\n",
      "Epoch 035 | Val Acc: 0.8653\n",
      "Epoch 036 | Val Acc: 0.8734\n",
      "Epoch 037 | Val Acc: 0.8645\n",
      "Epoch 038 | Val Acc: 0.8613\n",
      "Epoch 039 | Val Acc: 0.8556\n",
      "Epoch 040 | Val Acc: 0.8597\n",
      "Epoch 041 | Val Acc: 0.8661\n",
      "Epoch 042 | Val Acc: 0.8661\n",
      "Epoch 043 | Val Acc: 0.8589\n",
      "Epoch 044 | Val Acc: 0.8815\n",
      "Epoch 045 | Val Acc: 0.8702\n",
      "Epoch 046 | Val Acc: 0.8774\n",
      "Epoch 047 | Val Acc: 0.8798\n",
      "Epoch 048 | Val Acc: 0.8637\n",
      "Epoch 049 | Val Acc: 0.8718\n",
      "Epoch 050 | Val Acc: 0.8702\n",
      "Epoch 051 | Val Acc: 0.8548\n",
      "Epoch 052 | Val Acc: 0.8484\n",
      "Epoch 053 | Val Acc: 0.8476\n",
      "Epoch 054 | Val Acc: 0.8758\n",
      "Early stopping\n",
      "Split Accuracy: 0.8815\n",
      "\n",
      "----- SPLIT 7 -----\n",
      "Epoch 001 | Val Acc: 0.0234\n",
      "Epoch 002 | Val Acc: 0.0790\n",
      "Epoch 003 | Val Acc: 0.3016\n",
      "Epoch 004 | Val Acc: 0.4750\n",
      "Epoch 005 | Val Acc: 0.5927\n",
      "Epoch 006 | Val Acc: 0.6419\n",
      "Epoch 007 | Val Acc: 0.7121\n",
      "Epoch 008 | Val Acc: 0.7315\n",
      "Epoch 009 | Val Acc: 0.7605\n",
      "Epoch 010 | Val Acc: 0.7831\n",
      "Epoch 011 | Val Acc: 0.7855\n",
      "Epoch 012 | Val Acc: 0.8185\n",
      "Epoch 013 | Val Acc: 0.8097\n",
      "Epoch 014 | Val Acc: 0.8323\n",
      "Epoch 015 | Val Acc: 0.8339\n",
      "Epoch 016 | Val Acc: 0.8371\n",
      "Epoch 017 | Val Acc: 0.8371\n",
      "Epoch 018 | Val Acc: 0.8339\n",
      "Epoch 019 | Val Acc: 0.8500\n",
      "Epoch 020 | Val Acc: 0.8589\n",
      "Epoch 021 | Val Acc: 0.8637\n",
      "Epoch 022 | Val Acc: 0.8629\n",
      "Epoch 023 | Val Acc: 0.8677\n",
      "Epoch 024 | Val Acc: 0.8597\n",
      "Epoch 025 | Val Acc: 0.8750\n",
      "Epoch 026 | Val Acc: 0.8710\n",
      "Epoch 027 | Val Acc: 0.8806\n",
      "Epoch 028 | Val Acc: 0.8710\n",
      "Epoch 029 | Val Acc: 0.8823\n",
      "Epoch 030 | Val Acc: 0.8798\n",
      "Epoch 031 | Val Acc: 0.8815\n",
      "Epoch 032 | Val Acc: 0.8782\n",
      "Epoch 033 | Val Acc: 0.8766\n",
      "Epoch 034 | Val Acc: 0.8911\n",
      "Epoch 035 | Val Acc: 0.8855\n",
      "Epoch 036 | Val Acc: 0.8798\n",
      "Epoch 037 | Val Acc: 0.8911\n",
      "Epoch 038 | Val Acc: 0.8702\n",
      "Epoch 039 | Val Acc: 0.8823\n",
      "Epoch 040 | Val Acc: 0.8887\n",
      "Epoch 041 | Val Acc: 0.8944\n",
      "Epoch 042 | Val Acc: 0.8895\n",
      "Epoch 043 | Val Acc: 0.8903\n",
      "Epoch 044 | Val Acc: 0.9000\n",
      "Epoch 045 | Val Acc: 0.8798\n",
      "Epoch 046 | Val Acc: 0.8855\n",
      "Epoch 047 | Val Acc: 0.9008\n",
      "Epoch 048 | Val Acc: 0.8952\n",
      "Epoch 049 | Val Acc: 0.8976\n",
      "Epoch 050 | Val Acc: 0.8919\n",
      "Epoch 051 | Val Acc: 0.8879\n",
      "Epoch 052 | Val Acc: 0.9048\n",
      "Epoch 053 | Val Acc: 0.8960\n",
      "Epoch 054 | Val Acc: 0.8919\n",
      "Epoch 055 | Val Acc: 0.8984\n",
      "Epoch 056 | Val Acc: 0.9016\n",
      "Epoch 057 | Val Acc: 0.8960\n",
      "Epoch 058 | Val Acc: 0.9016\n",
      "Epoch 059 | Val Acc: 0.8944\n",
      "Epoch 060 | Val Acc: 0.8960\n",
      "Epoch 061 | Val Acc: 0.8734\n",
      "Epoch 062 | Val Acc: 0.7847\n",
      "Early stopping\n",
      "Split Accuracy: 0.9048\n",
      "\n",
      "----- SPLIT 8 -----\n",
      "Epoch 001 | Val Acc: 0.0282\n",
      "Epoch 002 | Val Acc: 0.0669\n",
      "Epoch 003 | Val Acc: 0.2685\n",
      "Epoch 004 | Val Acc: 0.4419\n",
      "Epoch 005 | Val Acc: 0.5621\n",
      "Epoch 006 | Val Acc: 0.6113\n",
      "Epoch 007 | Val Acc: 0.6758\n",
      "Epoch 008 | Val Acc: 0.7000\n",
      "Epoch 009 | Val Acc: 0.7073\n",
      "Epoch 010 | Val Acc: 0.7419\n",
      "Epoch 011 | Val Acc: 0.7218\n",
      "Epoch 012 | Val Acc: 0.7540\n",
      "Epoch 013 | Val Acc: 0.7798\n",
      "Epoch 014 | Val Acc: 0.7766\n",
      "Epoch 015 | Val Acc: 0.7734\n",
      "Epoch 016 | Val Acc: 0.7863\n",
      "Epoch 017 | Val Acc: 0.7935\n",
      "Epoch 018 | Val Acc: 0.7879\n",
      "Epoch 019 | Val Acc: 0.8194\n",
      "Epoch 020 | Val Acc: 0.8210\n",
      "Epoch 021 | Val Acc: 0.8113\n",
      "Epoch 022 | Val Acc: 0.8202\n",
      "Epoch 023 | Val Acc: 0.8315\n",
      "Epoch 024 | Val Acc: 0.8306\n",
      "Epoch 025 | Val Acc: 0.8266\n",
      "Epoch 026 | Val Acc: 0.8282\n",
      "Epoch 027 | Val Acc: 0.8371\n",
      "Epoch 028 | Val Acc: 0.8258\n",
      "Epoch 029 | Val Acc: 0.8290\n",
      "Epoch 030 | Val Acc: 0.8468\n",
      "Epoch 031 | Val Acc: 0.8468\n",
      "Epoch 032 | Val Acc: 0.8452\n",
      "Epoch 033 | Val Acc: 0.8435\n",
      "Epoch 034 | Val Acc: 0.8452\n",
      "Epoch 035 | Val Acc: 0.8532\n",
      "Epoch 036 | Val Acc: 0.8484\n",
      "Epoch 037 | Val Acc: 0.8306\n",
      "Epoch 038 | Val Acc: 0.8581\n",
      "Epoch 039 | Val Acc: 0.8476\n",
      "Epoch 040 | Val Acc: 0.8476\n",
      "Epoch 041 | Val Acc: 0.8484\n",
      "Epoch 042 | Val Acc: 0.8524\n",
      "Epoch 043 | Val Acc: 0.8556\n",
      "Epoch 044 | Val Acc: 0.8605\n",
      "Epoch 045 | Val Acc: 0.8524\n",
      "Epoch 046 | Val Acc: 0.8460\n",
      "Epoch 047 | Val Acc: 0.8589\n",
      "Epoch 048 | Val Acc: 0.8540\n",
      "Epoch 049 | Val Acc: 0.8613\n",
      "Epoch 050 | Val Acc: 0.8645\n",
      "Epoch 051 | Val Acc: 0.8484\n",
      "Epoch 052 | Val Acc: 0.8435\n",
      "Epoch 053 | Val Acc: 0.8573\n",
      "Epoch 054 | Val Acc: 0.8661\n",
      "Epoch 055 | Val Acc: 0.8726\n",
      "Epoch 056 | Val Acc: 0.8637\n",
      "Epoch 057 | Val Acc: 0.8556\n",
      "Epoch 058 | Val Acc: 0.8669\n",
      "Epoch 059 | Val Acc: 0.8621\n",
      "Epoch 060 | Val Acc: 0.8597\n",
      "Epoch 061 | Val Acc: 0.8597\n",
      "Epoch 062 | Val Acc: 0.8629\n",
      "Epoch 063 | Val Acc: 0.8677\n",
      "Epoch 064 | Val Acc: 0.8565\n",
      "Epoch 065 | Val Acc: 0.8573\n",
      "Early stopping\n",
      "Split Accuracy: 0.8726\n",
      "\n",
      "----- SPLIT 9 -----\n",
      "Epoch 001 | Val Acc: 0.0177\n",
      "Epoch 002 | Val Acc: 0.0315\n",
      "Epoch 003 | Val Acc: 0.0750\n",
      "Epoch 004 | Val Acc: 0.2492\n",
      "Epoch 005 | Val Acc: 0.3839\n",
      "Epoch 006 | Val Acc: 0.4661\n",
      "Epoch 007 | Val Acc: 0.5468\n",
      "Epoch 008 | Val Acc: 0.6032\n",
      "Epoch 009 | Val Acc: 0.6556\n",
      "Epoch 010 | Val Acc: 0.7081\n",
      "Epoch 011 | Val Acc: 0.7282\n",
      "Epoch 012 | Val Acc: 0.7411\n",
      "Epoch 013 | Val Acc: 0.7573\n",
      "Epoch 014 | Val Acc: 0.7685\n",
      "Epoch 015 | Val Acc: 0.7806\n",
      "Epoch 016 | Val Acc: 0.7839\n",
      "Epoch 017 | Val Acc: 0.8016\n",
      "Epoch 018 | Val Acc: 0.8040\n",
      "Epoch 019 | Val Acc: 0.8065\n",
      "Epoch 020 | Val Acc: 0.8161\n",
      "Epoch 021 | Val Acc: 0.8202\n",
      "Epoch 022 | Val Acc: 0.8056\n",
      "Epoch 023 | Val Acc: 0.8234\n",
      "Epoch 024 | Val Acc: 0.8290\n",
      "Epoch 025 | Val Acc: 0.8194\n",
      "Epoch 026 | Val Acc: 0.8266\n",
      "Epoch 027 | Val Acc: 0.8242\n",
      "Epoch 028 | Val Acc: 0.8274\n",
      "Epoch 029 | Val Acc: 0.8306\n",
      "Epoch 030 | Val Acc: 0.8266\n",
      "Epoch 031 | Val Acc: 0.8484\n",
      "Epoch 032 | Val Acc: 0.8419\n",
      "Epoch 033 | Val Acc: 0.8532\n",
      "Epoch 034 | Val Acc: 0.8379\n",
      "Epoch 035 | Val Acc: 0.8306\n",
      "Epoch 036 | Val Acc: 0.8435\n",
      "Epoch 037 | Val Acc: 0.8379\n",
      "Epoch 038 | Val Acc: 0.8460\n",
      "Epoch 039 | Val Acc: 0.8419\n",
      "Epoch 040 | Val Acc: 0.8476\n",
      "Epoch 041 | Val Acc: 0.8556\n",
      "Epoch 042 | Val Acc: 0.8395\n",
      "Epoch 043 | Val Acc: 0.8500\n",
      "Epoch 044 | Val Acc: 0.8637\n",
      "Epoch 045 | Val Acc: 0.8556\n",
      "Epoch 046 | Val Acc: 0.8669\n",
      "Epoch 047 | Val Acc: 0.8661\n",
      "Epoch 048 | Val Acc: 0.8613\n",
      "Epoch 049 | Val Acc: 0.8718\n",
      "Epoch 050 | Val Acc: 0.8613\n",
      "Epoch 051 | Val Acc: 0.8629\n",
      "Epoch 052 | Val Acc: 0.8460\n",
      "Epoch 053 | Val Acc: 0.8629\n",
      "Epoch 054 | Val Acc: 0.8613\n",
      "Epoch 055 | Val Acc: 0.8395\n",
      "Epoch 056 | Val Acc: 0.8548\n",
      "Epoch 057 | Val Acc: 0.8113\n",
      "Epoch 058 | Val Acc: 0.8427\n",
      "Epoch 059 | Val Acc: 0.8573\n",
      "Early stopping\n",
      "Split Accuracy: 0.8718\n",
      "\n",
      "----- SPLIT 10 -----\n",
      "Epoch 001 | Val Acc: 0.0218\n",
      "Epoch 002 | Val Acc: 0.0806\n",
      "Epoch 003 | Val Acc: 0.3460\n",
      "Epoch 004 | Val Acc: 0.4694\n",
      "Epoch 005 | Val Acc: 0.5581\n",
      "Epoch 006 | Val Acc: 0.6218\n",
      "Epoch 007 | Val Acc: 0.6306\n",
      "Epoch 008 | Val Acc: 0.6863\n",
      "Epoch 009 | Val Acc: 0.7089\n",
      "Epoch 010 | Val Acc: 0.7524\n",
      "Epoch 011 | Val Acc: 0.7532\n",
      "Epoch 012 | Val Acc: 0.7774\n",
      "Epoch 013 | Val Acc: 0.7782\n",
      "Epoch 014 | Val Acc: 0.7855\n",
      "Epoch 015 | Val Acc: 0.7895\n",
      "Epoch 016 | Val Acc: 0.8113\n",
      "Epoch 017 | Val Acc: 0.7879\n",
      "Epoch 018 | Val Acc: 0.8177\n",
      "Epoch 019 | Val Acc: 0.8145\n",
      "Epoch 020 | Val Acc: 0.8194\n",
      "Epoch 021 | Val Acc: 0.8065\n",
      "Epoch 022 | Val Acc: 0.8113\n",
      "Epoch 023 | Val Acc: 0.8250\n",
      "Epoch 024 | Val Acc: 0.8363\n",
      "Epoch 025 | Val Acc: 0.8218\n",
      "Epoch 026 | Val Acc: 0.8315\n",
      "Epoch 027 | Val Acc: 0.8218\n",
      "Epoch 028 | Val Acc: 0.8363\n",
      "Epoch 029 | Val Acc: 0.8403\n",
      "Epoch 030 | Val Acc: 0.8266\n",
      "Epoch 031 | Val Acc: 0.8395\n",
      "Epoch 032 | Val Acc: 0.8242\n",
      "Epoch 033 | Val Acc: 0.8250\n",
      "Epoch 034 | Val Acc: 0.8274\n",
      "Epoch 035 | Val Acc: 0.8492\n",
      "Epoch 036 | Val Acc: 0.8476\n",
      "Epoch 037 | Val Acc: 0.8476\n",
      "Epoch 038 | Val Acc: 0.8589\n",
      "Epoch 039 | Val Acc: 0.8492\n",
      "Epoch 040 | Val Acc: 0.8589\n",
      "Epoch 041 | Val Acc: 0.8452\n",
      "Epoch 042 | Val Acc: 0.8476\n",
      "Epoch 043 | Val Acc: 0.8476\n",
      "Epoch 044 | Val Acc: 0.8355\n",
      "Epoch 045 | Val Acc: 0.8565\n",
      "Epoch 046 | Val Acc: 0.8524\n",
      "Epoch 047 | Val Acc: 0.8532\n",
      "Epoch 048 | Val Acc: 0.8468\n",
      "Early stopping\n",
      "Split Accuracy: 0.8589\n",
      "\n",
      ">>> Average Accuracy (NTXent): 0.8706\n",
      "\n",
      "================ FINAL RESULTS ================\n",
      "Triplet_L2           : 0.8643\n",
      "Triplet_Cosine       : 0.8591\n",
      "NTXent               : 0.8706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, distances\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------- Dataset ----------------\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, x_path, y_path):\n",
    "        self.X = np.load(x_path)\n",
    "        self.y = np.argmax(np.load(y_path), axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# ---------------- CNN Encoder (LEARNED VERSION) ----------------\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(6, 64, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "\n",
    "            nn.Conv1d(64, 256, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(256, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)          # (B, C, T)\n",
    "        x = self.features(x).squeeze(-1)\n",
    "        z = self.fc(x)\n",
    "        return z                        #  no normalize here\n",
    "\n",
    "# ---------------- Full Model ----------------\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_classes=62):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(emb_dim)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        logits = self.classifier(z)\n",
    "        return z, logits\n",
    "\n",
    "# ---------------- Training ----------------\n",
    "def train_and_eval(model, train_loader, test_loader,\n",
    "                   metric_loss, miner,\n",
    "                   epochs=500, patience=10):\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "    best_acc = -1\n",
    "    patience_ctr = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            embeddings, logits = model(x)\n",
    "\n",
    "            indices_tuple = miner(embeddings, y)\n",
    "            loss = (\n",
    "                metric_loss(embeddings, y, indices_tuple)\n",
    "                + ce_loss(logits, y)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # -------- Evaluation --------\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, logits = model(x)\n",
    "                preds = logits.argmax(1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1:03d} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_acc\n",
    "\n",
    "# ---------------- Metric Losses (REFERENCE-STYLE) ----------------\n",
    "encoder_losses = {\n",
    "    \"Triplet_L2\": losses.TripletMarginLoss(\n",
    "        margin=0.2,\n",
    "        distance=distances.LpDistance(normalize_embeddings=True, p=2, power=2),\n",
    "        smooth_loss=True\n",
    "    ),\n",
    "    \"Triplet_Cosine\": losses.TripletMarginLoss(\n",
    "        margin=0.1,\n",
    "        distance=distances.CosineSimilarity(),\n",
    "        smooth_loss=True\n",
    "    ),\n",
    "    \"NTXent\": losses.NTXentLoss(temperature=0.2)\n",
    "}\n",
    "\n",
    "# ---------------- Miner (KEY FIX) ----------------\n",
    "miner = miners.BatchEasyHardMiner(\n",
    "    pos_strategy=\"semihard\",\n",
    "    neg_strategy=\"hard\"\n",
    ")\n",
    "\n",
    "# ---------------- Experiment Loop ----------------\n",
    "BASE = \"models/Data/Data/62_classes/UserDependenet\"\n",
    "final_results = {}\n",
    "\n",
    "for loss_name, metric_loss in encoder_losses.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Encoder Loss: {loss_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for split in range(1, 11):\n",
    "        print(f\"\\n----- SPLIT {split} -----\")\n",
    "\n",
    "        train_ds = EMGDataset(\n",
    "            f\"{BASE}/Train/X_train_{split}.npy\",\n",
    "            f\"{BASE}/Train/y_train_{split}.npy\"\n",
    "        )\n",
    "        test_ds = EMGDataset(\n",
    "            f\"{BASE}/Test/X_test_{split}.npy\",\n",
    "            f\"{BASE}/Test/y_test_{split}.npy\"\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
    "\n",
    "        model = FullModel().to(device)\n",
    "\n",
    "        acc = train_and_eval(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            metric_loss,\n",
    "            miner,\n",
    "            epochs=500,\n",
    "            patience=10\n",
    "        )\n",
    "\n",
    "        print(f\"Split Accuracy: {acc:.4f}\")\n",
    "        accs.append(acc)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    final_results[loss_name] = np.mean(accs)\n",
    "    print(f\"\\n>>> Average Accuracy ({loss_name}): {final_results[loss_name]:.4f}\")\n",
    "\n",
    "print(\"\\n================ FINAL RESULTS ================\")\n",
    "for k, v in final_results.items():\n",
    "    print(f\"{k:20s} : {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad4869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "========== Triplet_L2 ==========\n",
      "Epoch 001 | Val Acc: 0.0476\n",
      "Epoch 002 | Val Acc: 0.0758\n",
      "Epoch 003 | Val Acc: 0.1266\n",
      "Epoch 004 | Val Acc: 0.1798\n",
      "Epoch 005 | Val Acc: 0.2411\n",
      "Epoch 006 | Val Acc: 0.3323\n",
      "Epoch 007 | Val Acc: 0.3831\n",
      "Epoch 008 | Val Acc: 0.3887\n",
      "Epoch 009 | Val Acc: 0.4645\n",
      "Epoch 010 | Val Acc: 0.5161\n",
      "Epoch 011 | Val Acc: 0.4798\n",
      "Epoch 012 | Val Acc: 0.5403\n",
      "Epoch 013 | Val Acc: 0.5839\n",
      "Epoch 014 | Val Acc: 0.5976\n",
      "Epoch 015 | Val Acc: 0.6008\n",
      "Epoch 016 | Val Acc: 0.6250\n",
      "Epoch 017 | Val Acc: 0.6323\n",
      "Epoch 018 | Val Acc: 0.6476\n",
      "Epoch 019 | Val Acc: 0.6484\n",
      "Epoch 020 | Val Acc: 0.6387\n",
      "Epoch 021 | Val Acc: 0.6702\n",
      "Epoch 022 | Val Acc: 0.6782\n",
      "Epoch 023 | Val Acc: 0.6887\n",
      "Epoch 024 | Val Acc: 0.6871\n",
      "Epoch 025 | Val Acc: 0.6758\n",
      "Epoch 026 | Val Acc: 0.6823\n",
      "Epoch 027 | Val Acc: 0.6839\n",
      "Epoch 028 | Val Acc: 0.7040\n",
      "Epoch 029 | Val Acc: 0.6968\n",
      "Epoch 030 | Val Acc: 0.7081\n",
      "Epoch 031 | Val Acc: 0.7040\n",
      "Epoch 032 | Val Acc: 0.7016\n",
      "Epoch 033 | Val Acc: 0.6911\n",
      "Epoch 034 | Val Acc: 0.7065\n",
      "Epoch 035 | Val Acc: 0.7185\n",
      "Epoch 036 | Val Acc: 0.7137\n",
      "Epoch 037 | Val Acc: 0.7153\n",
      "Epoch 038 | Val Acc: 0.7266\n",
      "Epoch 039 | Val Acc: 0.7105\n",
      "Epoch 040 | Val Acc: 0.7089\n",
      "Epoch 041 | Val Acc: 0.7105\n",
      "Epoch 042 | Val Acc: 0.7008\n",
      "Epoch 043 | Val Acc: 0.7040\n",
      "Epoch 044 | Val Acc: 0.7161\n",
      "Epoch 045 | Val Acc: 0.7081\n",
      "Epoch 046 | Val Acc: 0.7290\n",
      "Epoch 047 | Val Acc: 0.7194\n",
      "Epoch 048 | Val Acc: 0.7177\n",
      "Epoch 049 | Val Acc: 0.7129\n",
      "Epoch 050 | Val Acc: 0.7121\n",
      "Epoch 051 | Val Acc: 0.7266\n",
      "Epoch 052 | Val Acc: 0.7032\n",
      "Epoch 053 | Val Acc: 0.7194\n",
      "Epoch 054 | Val Acc: 0.7234\n",
      "Epoch 055 | Val Acc: 0.7185\n",
      "Epoch 056 | Val Acc: 0.7194\n",
      "Epoch 057 | Val Acc: 0.7306\n",
      "Epoch 058 | Val Acc: 0.7145\n",
      "Epoch 059 | Val Acc: 0.7218\n",
      "Epoch 060 | Val Acc: 0.7323\n",
      "Epoch 061 | Val Acc: 0.7089\n",
      "Epoch 062 | Val Acc: 0.7169\n",
      "Epoch 063 | Val Acc: 0.7234\n",
      "Epoch 064 | Val Acc: 0.7161\n",
      "Epoch 065 | Val Acc: 0.7210\n",
      "Epoch 066 | Val Acc: 0.7218\n",
      "Epoch 067 | Val Acc: 0.7169\n",
      "Epoch 068 | Val Acc: 0.7226\n",
      "Epoch 069 | Val Acc: 0.7194\n",
      "Epoch 070 | Val Acc: 0.7234\n",
      "Epoch 071 | Val Acc: 0.7210\n",
      "Epoch 072 | Val Acc: 0.7185\n",
      "Early stopping\n",
      "Split 1 Acc: 0.7323\n",
      "Epoch 001 | Val Acc: 0.0500\n",
      "Epoch 002 | Val Acc: 0.0903\n",
      "Epoch 003 | Val Acc: 0.1677\n",
      "Epoch 004 | Val Acc: 0.2355\n",
      "Epoch 005 | Val Acc: 0.2742\n",
      "Epoch 006 | Val Acc: 0.3742\n",
      "Epoch 007 | Val Acc: 0.4145\n",
      "Epoch 008 | Val Acc: 0.4919\n",
      "Epoch 009 | Val Acc: 0.5218\n",
      "Epoch 010 | Val Acc: 0.5790\n",
      "Epoch 011 | Val Acc: 0.6000\n",
      "Epoch 012 | Val Acc: 0.6226\n",
      "Epoch 013 | Val Acc: 0.6323\n",
      "Epoch 014 | Val Acc: 0.6702\n",
      "Epoch 015 | Val Acc: 0.6726\n",
      "Epoch 016 | Val Acc: 0.6685\n",
      "Epoch 017 | Val Acc: 0.6944\n",
      "Epoch 018 | Val Acc: 0.6944\n",
      "Epoch 019 | Val Acc: 0.7153\n",
      "Epoch 020 | Val Acc: 0.7185\n",
      "Epoch 021 | Val Acc: 0.7226\n",
      "Epoch 022 | Val Acc: 0.7290\n",
      "Epoch 023 | Val Acc: 0.7274\n",
      "Epoch 024 | Val Acc: 0.7387\n",
      "Epoch 025 | Val Acc: 0.7315\n",
      "Epoch 026 | Val Acc: 0.7258\n",
      "Epoch 027 | Val Acc: 0.7468\n",
      "Epoch 028 | Val Acc: 0.7371\n",
      "Epoch 029 | Val Acc: 0.7565\n",
      "Epoch 030 | Val Acc: 0.7540\n",
      "Epoch 031 | Val Acc: 0.7476\n",
      "Epoch 032 | Val Acc: 0.7685\n",
      "Epoch 033 | Val Acc: 0.7532\n",
      "Epoch 034 | Val Acc: 0.7427\n",
      "Epoch 035 | Val Acc: 0.7532\n",
      "Epoch 036 | Val Acc: 0.7750\n",
      "Epoch 037 | Val Acc: 0.7734\n",
      "Epoch 038 | Val Acc: 0.7621\n",
      "Epoch 039 | Val Acc: 0.7839\n",
      "Epoch 040 | Val Acc: 0.7710\n",
      "Epoch 041 | Val Acc: 0.7750\n",
      "Epoch 042 | Val Acc: 0.7847\n",
      "Epoch 043 | Val Acc: 0.7718\n",
      "Epoch 044 | Val Acc: 0.7815\n",
      "Epoch 045 | Val Acc: 0.7798\n",
      "Epoch 046 | Val Acc: 0.7750\n",
      "Epoch 047 | Val Acc: 0.7734\n",
      "Epoch 048 | Val Acc: 0.7831\n",
      "Epoch 049 | Val Acc: 0.7798\n",
      "Epoch 050 | Val Acc: 0.7935\n",
      "Epoch 051 | Val Acc: 0.7726\n",
      "Epoch 052 | Val Acc: 0.7806\n",
      "Epoch 053 | Val Acc: 0.7815\n",
      "Epoch 054 | Val Acc: 0.7855\n",
      "Epoch 055 | Val Acc: 0.7944\n",
      "Epoch 056 | Val Acc: 0.7581\n",
      "Epoch 057 | Val Acc: 0.7839\n",
      "Epoch 058 | Val Acc: 0.7798\n",
      "Epoch 059 | Val Acc: 0.7968\n",
      "Epoch 060 | Val Acc: 0.7653\n",
      "Epoch 061 | Val Acc: 0.7798\n",
      "Epoch 062 | Val Acc: 0.7887\n",
      "Epoch 063 | Val Acc: 0.7831\n",
      "Epoch 064 | Val Acc: 0.7903\n",
      "Epoch 065 | Val Acc: 0.7879\n",
      "Epoch 066 | Val Acc: 0.7798\n",
      "Epoch 067 | Val Acc: 0.7952\n",
      "Epoch 068 | Val Acc: 0.7895\n",
      "Epoch 069 | Val Acc: 0.7815\n",
      "Epoch 070 | Val Acc: 0.7815\n",
      "Epoch 071 | Val Acc: 0.7798\n",
      "Early stopping\n",
      "Split 2 Acc: 0.7968\n",
      "Epoch 001 | Val Acc: 0.0258\n",
      "Epoch 002 | Val Acc: 0.1081\n",
      "Epoch 003 | Val Acc: 0.1702\n",
      "Epoch 004 | Val Acc: 0.2508\n",
      "Epoch 005 | Val Acc: 0.3121\n",
      "Epoch 006 | Val Acc: 0.3863\n",
      "Epoch 007 | Val Acc: 0.4460\n",
      "Epoch 008 | Val Acc: 0.5008\n",
      "Epoch 009 | Val Acc: 0.5387\n",
      "Epoch 010 | Val Acc: 0.5758\n",
      "Epoch 011 | Val Acc: 0.5839\n",
      "Epoch 012 | Val Acc: 0.6121\n",
      "Epoch 013 | Val Acc: 0.6258\n",
      "Epoch 014 | Val Acc: 0.6234\n",
      "Epoch 015 | Val Acc: 0.6556\n",
      "Epoch 016 | Val Acc: 0.6468\n",
      "Epoch 017 | Val Acc: 0.6774\n",
      "Epoch 018 | Val Acc: 0.7097\n",
      "Epoch 019 | Val Acc: 0.6976\n",
      "Epoch 020 | Val Acc: 0.7129\n",
      "Epoch 021 | Val Acc: 0.6823\n",
      "Epoch 022 | Val Acc: 0.7185\n",
      "Epoch 023 | Val Acc: 0.7234\n",
      "Epoch 024 | Val Acc: 0.7323\n",
      "Epoch 025 | Val Acc: 0.7347\n",
      "Epoch 026 | Val Acc: 0.7242\n",
      "Epoch 027 | Val Acc: 0.7444\n",
      "Epoch 028 | Val Acc: 0.7508\n",
      "Epoch 029 | Val Acc: 0.7653\n",
      "Epoch 030 | Val Acc: 0.7645\n",
      "Epoch 031 | Val Acc: 0.7540\n",
      "Epoch 032 | Val Acc: 0.7815\n",
      "Epoch 033 | Val Acc: 0.7589\n",
      "Epoch 034 | Val Acc: 0.7677\n",
      "Epoch 035 | Val Acc: 0.7532\n",
      "Epoch 036 | Val Acc: 0.7694\n",
      "Epoch 037 | Val Acc: 0.7710\n",
      "Epoch 038 | Val Acc: 0.7653\n",
      "Epoch 039 | Val Acc: 0.7774\n",
      "Epoch 040 | Val Acc: 0.7927\n",
      "Epoch 041 | Val Acc: 0.7742\n",
      "Epoch 042 | Val Acc: 0.7645\n",
      "Epoch 043 | Val Acc: 0.7823\n",
      "Epoch 044 | Val Acc: 0.7774\n",
      "Epoch 045 | Val Acc: 0.7806\n",
      "Epoch 046 | Val Acc: 0.7742\n",
      "Epoch 047 | Val Acc: 0.8024\n",
      "Epoch 048 | Val Acc: 0.7919\n",
      "Epoch 049 | Val Acc: 0.7847\n",
      "Epoch 050 | Val Acc: 0.7871\n",
      "Epoch 051 | Val Acc: 0.7806\n",
      "Epoch 052 | Val Acc: 0.7790\n",
      "Epoch 053 | Val Acc: 0.7806\n",
      "Epoch 054 | Val Acc: 0.7774\n",
      "Epoch 055 | Val Acc: 0.7806\n",
      "Epoch 056 | Val Acc: 0.7887\n",
      "Epoch 057 | Val Acc: 0.7895\n",
      "Epoch 058 | Val Acc: 0.7831\n",
      "Epoch 059 | Val Acc: 0.7847\n",
      "Early stopping\n",
      "Split 3 Acc: 0.8024\n",
      "Epoch 001 | Val Acc: 0.0637\n",
      "Epoch 002 | Val Acc: 0.1000\n",
      "Epoch 003 | Val Acc: 0.1589\n",
      "Epoch 004 | Val Acc: 0.2565\n",
      "Epoch 005 | Val Acc: 0.3040\n",
      "Epoch 006 | Val Acc: 0.3637\n",
      "Epoch 007 | Val Acc: 0.4129\n",
      "Epoch 008 | Val Acc: 0.5000\n",
      "Epoch 009 | Val Acc: 0.5266\n",
      "Epoch 010 | Val Acc: 0.5500\n",
      "Epoch 011 | Val Acc: 0.5984\n",
      "Epoch 012 | Val Acc: 0.6379\n",
      "Epoch 013 | Val Acc: 0.6363\n",
      "Epoch 014 | Val Acc: 0.6613\n",
      "Epoch 015 | Val Acc: 0.6839\n",
      "Epoch 016 | Val Acc: 0.7040\n",
      "Epoch 017 | Val Acc: 0.7194\n",
      "Epoch 018 | Val Acc: 0.7290\n",
      "Epoch 019 | Val Acc: 0.7484\n",
      "Epoch 020 | Val Acc: 0.7347\n",
      "Epoch 021 | Val Acc: 0.7581\n",
      "Epoch 022 | Val Acc: 0.7548\n",
      "Epoch 023 | Val Acc: 0.7645\n",
      "Epoch 024 | Val Acc: 0.7621\n",
      "Epoch 025 | Val Acc: 0.7774\n",
      "Epoch 026 | Val Acc: 0.7427\n",
      "Epoch 027 | Val Acc: 0.7847\n",
      "Epoch 028 | Val Acc: 0.7879\n",
      "Epoch 029 | Val Acc: 0.7887\n",
      "Epoch 030 | Val Acc: 0.7855\n",
      "Epoch 031 | Val Acc: 0.7879\n",
      "Epoch 032 | Val Acc: 0.7758\n",
      "Epoch 033 | Val Acc: 0.7871\n",
      "Epoch 034 | Val Acc: 0.8016\n",
      "Epoch 035 | Val Acc: 0.7984\n",
      "Epoch 036 | Val Acc: 0.8089\n",
      "Epoch 037 | Val Acc: 0.8024\n",
      "Epoch 038 | Val Acc: 0.8032\n",
      "Epoch 039 | Val Acc: 0.8008\n",
      "Epoch 040 | Val Acc: 0.7984\n",
      "Epoch 041 | Val Acc: 0.8242\n",
      "Epoch 042 | Val Acc: 0.8137\n",
      "Epoch 043 | Val Acc: 0.8105\n",
      "Epoch 044 | Val Acc: 0.8169\n",
      "Epoch 045 | Val Acc: 0.8024\n",
      "Epoch 046 | Val Acc: 0.8242\n",
      "Epoch 047 | Val Acc: 0.8113\n",
      "Epoch 048 | Val Acc: 0.7984\n",
      "Epoch 049 | Val Acc: 0.8113\n",
      "Epoch 050 | Val Acc: 0.8161\n",
      "Epoch 051 | Val Acc: 0.8250\n",
      "Epoch 052 | Val Acc: 0.8073\n",
      "Epoch 053 | Val Acc: 0.8242\n",
      "Epoch 054 | Val Acc: 0.8306\n",
      "Epoch 055 | Val Acc: 0.8185\n",
      "Epoch 056 | Val Acc: 0.8266\n",
      "Epoch 057 | Val Acc: 0.8323\n",
      "Epoch 058 | Val Acc: 0.8282\n",
      "Epoch 059 | Val Acc: 0.8105\n",
      "Epoch 060 | Val Acc: 0.8210\n",
      "Epoch 061 | Val Acc: 0.8274\n",
      "Epoch 062 | Val Acc: 0.8363\n",
      "Epoch 063 | Val Acc: 0.8129\n",
      "Epoch 064 | Val Acc: 0.8048\n",
      "Epoch 065 | Val Acc: 0.8266\n",
      "Epoch 066 | Val Acc: 0.8371\n",
      "Epoch 067 | Val Acc: 0.8258\n",
      "Epoch 068 | Val Acc: 0.8218\n",
      "Epoch 069 | Val Acc: 0.8153\n",
      "Epoch 070 | Val Acc: 0.8129\n",
      "Epoch 071 | Val Acc: 0.8274\n",
      "Epoch 072 | Val Acc: 0.8218\n",
      "Epoch 073 | Val Acc: 0.8298\n",
      "Epoch 074 | Val Acc: 0.8306\n",
      "Epoch 075 | Val Acc: 0.8290\n",
      "Epoch 076 | Val Acc: 0.8323\n",
      "Epoch 077 | Val Acc: 0.8323\n",
      "Epoch 078 | Val Acc: 0.8258\n",
      "Early stopping\n",
      "Split 4 Acc: 0.8371\n",
      "Epoch 001 | Val Acc: 0.0435\n",
      "Epoch 002 | Val Acc: 0.0879\n",
      "Epoch 003 | Val Acc: 0.1919\n",
      "Epoch 004 | Val Acc: 0.2395\n",
      "Epoch 005 | Val Acc: 0.3395\n",
      "Epoch 006 | Val Acc: 0.4048\n",
      "Epoch 007 | Val Acc: 0.4516\n",
      "Epoch 008 | Val Acc: 0.5048\n",
      "Epoch 009 | Val Acc: 0.5121\n",
      "Epoch 010 | Val Acc: 0.5806\n",
      "Epoch 011 | Val Acc: 0.5944\n",
      "Epoch 012 | Val Acc: 0.6145\n",
      "Epoch 013 | Val Acc: 0.6524\n",
      "Epoch 014 | Val Acc: 0.6645\n",
      "Epoch 015 | Val Acc: 0.7040\n",
      "Epoch 016 | Val Acc: 0.7016\n",
      "Epoch 017 | Val Acc: 0.7065\n",
      "Epoch 018 | Val Acc: 0.7218\n",
      "Epoch 019 | Val Acc: 0.7387\n",
      "Epoch 020 | Val Acc: 0.7468\n",
      "Epoch 021 | Val Acc: 0.7645\n",
      "Epoch 022 | Val Acc: 0.7548\n",
      "Epoch 023 | Val Acc: 0.7685\n",
      "Epoch 024 | Val Acc: 0.7782\n",
      "Epoch 025 | Val Acc: 0.7855\n",
      "Epoch 026 | Val Acc: 0.7911\n",
      "Epoch 027 | Val Acc: 0.7911\n",
      "Epoch 028 | Val Acc: 0.7847\n",
      "Epoch 029 | Val Acc: 0.7976\n",
      "Epoch 030 | Val Acc: 0.7685\n",
      "Epoch 031 | Val Acc: 0.7806\n",
      "Epoch 032 | Val Acc: 0.7919\n",
      "Epoch 033 | Val Acc: 0.7992\n",
      "Epoch 034 | Val Acc: 0.8048\n",
      "Epoch 035 | Val Acc: 0.8056\n",
      "Epoch 036 | Val Acc: 0.8185\n",
      "Epoch 037 | Val Acc: 0.8097\n",
      "Epoch 038 | Val Acc: 0.8129\n",
      "Epoch 039 | Val Acc: 0.8129\n",
      "Epoch 040 | Val Acc: 0.8153\n",
      "Epoch 041 | Val Acc: 0.8121\n",
      "Epoch 042 | Val Acc: 0.8121\n",
      "Epoch 043 | Val Acc: 0.8161\n",
      "Epoch 044 | Val Acc: 0.8194\n",
      "Epoch 045 | Val Acc: 0.8242\n",
      "Epoch 046 | Val Acc: 0.8121\n",
      "Epoch 047 | Val Acc: 0.8129\n",
      "Epoch 048 | Val Acc: 0.8121\n",
      "Epoch 049 | Val Acc: 0.8089\n",
      "Epoch 050 | Val Acc: 0.8081\n",
      "Epoch 051 | Val Acc: 0.8194\n",
      "Epoch 052 | Val Acc: 0.8169\n",
      "Epoch 053 | Val Acc: 0.8048\n",
      "Epoch 054 | Val Acc: 0.8266\n",
      "Epoch 055 | Val Acc: 0.8185\n",
      "Epoch 056 | Val Acc: 0.8177\n",
      "Epoch 057 | Val Acc: 0.8218\n",
      "Epoch 058 | Val Acc: 0.8234\n",
      "Epoch 059 | Val Acc: 0.8306\n",
      "Epoch 060 | Val Acc: 0.8129\n",
      "Epoch 061 | Val Acc: 0.8032\n",
      "Epoch 062 | Val Acc: 0.8258\n",
      "Epoch 063 | Val Acc: 0.8274\n",
      "Epoch 064 | Val Acc: 0.8395\n",
      "Epoch 065 | Val Acc: 0.8218\n",
      "Epoch 066 | Val Acc: 0.8234\n",
      "Epoch 067 | Val Acc: 0.8089\n",
      "Epoch 068 | Val Acc: 0.8121\n",
      "Epoch 069 | Val Acc: 0.8161\n",
      "Epoch 070 | Val Acc: 0.8177\n",
      "Epoch 071 | Val Acc: 0.8185\n",
      "Epoch 072 | Val Acc: 0.8161\n",
      "Epoch 073 | Val Acc: 0.8258\n",
      "Epoch 074 | Val Acc: 0.8234\n",
      "Epoch 075 | Val Acc: 0.8234\n",
      "Epoch 076 | Val Acc: 0.8226\n",
      "Early stopping\n",
      "Split 5 Acc: 0.8395\n",
      "Epoch 001 | Val Acc: 0.0435\n",
      "Epoch 002 | Val Acc: 0.0718\n",
      "Epoch 003 | Val Acc: 0.1435\n",
      "Epoch 004 | Val Acc: 0.1782\n",
      "Epoch 005 | Val Acc: 0.2718\n",
      "Epoch 006 | Val Acc: 0.3331\n",
      "Epoch 007 | Val Acc: 0.4298\n",
      "Epoch 008 | Val Acc: 0.4508\n",
      "Epoch 009 | Val Acc: 0.5081\n",
      "Epoch 010 | Val Acc: 0.5460\n",
      "Epoch 011 | Val Acc: 0.6008\n",
      "Epoch 012 | Val Acc: 0.6226\n",
      "Epoch 013 | Val Acc: 0.6371\n",
      "Epoch 014 | Val Acc: 0.6331\n",
      "Epoch 015 | Val Acc: 0.6742\n",
      "Epoch 016 | Val Acc: 0.7008\n",
      "Epoch 017 | Val Acc: 0.7161\n",
      "Epoch 018 | Val Acc: 0.7129\n",
      "Epoch 019 | Val Acc: 0.7290\n",
      "Epoch 020 | Val Acc: 0.7395\n",
      "Epoch 021 | Val Acc: 0.7371\n",
      "Epoch 022 | Val Acc: 0.7597\n",
      "Epoch 023 | Val Acc: 0.7419\n",
      "Epoch 024 | Val Acc: 0.7677\n",
      "Epoch 025 | Val Acc: 0.7637\n",
      "Epoch 026 | Val Acc: 0.7758\n",
      "Epoch 027 | Val Acc: 0.7919\n",
      "Epoch 028 | Val Acc: 0.7927\n",
      "Epoch 029 | Val Acc: 0.7798\n",
      "Epoch 030 | Val Acc: 0.7968\n",
      "Epoch 031 | Val Acc: 0.7895\n",
      "Epoch 032 | Val Acc: 0.7871\n",
      "Epoch 033 | Val Acc: 0.7919\n",
      "Epoch 034 | Val Acc: 0.8089\n",
      "Epoch 035 | Val Acc: 0.8048\n",
      "Epoch 036 | Val Acc: 0.8065\n",
      "Epoch 037 | Val Acc: 0.8121\n",
      "Epoch 038 | Val Acc: 0.8040\n",
      "Epoch 039 | Val Acc: 0.8113\n",
      "Epoch 040 | Val Acc: 0.8129\n",
      "Epoch 041 | Val Acc: 0.8000\n",
      "Epoch 042 | Val Acc: 0.8081\n",
      "Epoch 043 | Val Acc: 0.8202\n",
      "Epoch 044 | Val Acc: 0.7976\n",
      "Epoch 045 | Val Acc: 0.8105\n",
      "Epoch 046 | Val Acc: 0.8202\n",
      "Epoch 047 | Val Acc: 0.8210\n",
      "Epoch 048 | Val Acc: 0.8234\n",
      "Epoch 049 | Val Acc: 0.8016\n",
      "Epoch 050 | Val Acc: 0.8145\n",
      "Epoch 051 | Val Acc: 0.8137\n",
      "Epoch 052 | Val Acc: 0.8048\n",
      "Epoch 053 | Val Acc: 0.8113\n",
      "Epoch 054 | Val Acc: 0.8137\n",
      "Epoch 055 | Val Acc: 0.8194\n",
      "Epoch 056 | Val Acc: 0.8105\n",
      "Epoch 057 | Val Acc: 0.8137\n",
      "Epoch 058 | Val Acc: 0.8137\n",
      "Epoch 059 | Val Acc: 0.8250\n",
      "Epoch 060 | Val Acc: 0.8210\n",
      "Epoch 061 | Val Acc: 0.8194\n",
      "Epoch 062 | Val Acc: 0.8056\n",
      "Epoch 063 | Val Acc: 0.8185\n",
      "Epoch 064 | Val Acc: 0.8218\n",
      "Epoch 065 | Val Acc: 0.8202\n",
      "Epoch 066 | Val Acc: 0.8056\n",
      "Epoch 067 | Val Acc: 0.8282\n",
      "Epoch 068 | Val Acc: 0.8250\n",
      "Epoch 069 | Val Acc: 0.8266\n",
      "Epoch 070 | Val Acc: 0.8145\n",
      "Epoch 071 | Val Acc: 0.8202\n",
      "Epoch 072 | Val Acc: 0.8331\n",
      "Epoch 073 | Val Acc: 0.8210\n",
      "Epoch 074 | Val Acc: 0.8226\n",
      "Epoch 075 | Val Acc: 0.8339\n",
      "Epoch 076 | Val Acc: 0.8194\n",
      "Epoch 077 | Val Acc: 0.8137\n",
      "Epoch 078 | Val Acc: 0.8315\n",
      "Epoch 079 | Val Acc: 0.8234\n",
      "Epoch 080 | Val Acc: 0.8258\n",
      "Epoch 081 | Val Acc: 0.8258\n",
      "Epoch 082 | Val Acc: 0.8258\n",
      "Epoch 083 | Val Acc: 0.8226\n",
      "Epoch 084 | Val Acc: 0.8153\n",
      "Epoch 085 | Val Acc: 0.8331\n",
      "Epoch 086 | Val Acc: 0.8121\n",
      "Epoch 087 | Val Acc: 0.8371\n",
      "Epoch 088 | Val Acc: 0.8323\n",
      "Epoch 089 | Val Acc: 0.8323\n",
      "Epoch 090 | Val Acc: 0.8258\n",
      "Epoch 091 | Val Acc: 0.8065\n",
      "Epoch 092 | Val Acc: 0.8258\n",
      "Epoch 093 | Val Acc: 0.8298\n",
      "Epoch 094 | Val Acc: 0.8282\n",
      "Epoch 095 | Val Acc: 0.8460\n",
      "Epoch 096 | Val Acc: 0.8306\n",
      "Epoch 097 | Val Acc: 0.8161\n",
      "Epoch 098 | Val Acc: 0.8218\n",
      "Epoch 099 | Val Acc: 0.8250\n",
      "Epoch 100 | Val Acc: 0.8185\n",
      "Epoch 101 | Val Acc: 0.8274\n",
      "Epoch 102 | Val Acc: 0.8218\n",
      "Epoch 103 | Val Acc: 0.8258\n",
      "Epoch 104 | Val Acc: 0.8315\n",
      "Epoch 105 | Val Acc: 0.8460\n",
      "Epoch 106 | Val Acc: 0.8331\n",
      "Epoch 107 | Val Acc: 0.8339\n",
      "Early stopping\n",
      "Split 6 Acc: 0.8460\n",
      "Epoch 001 | Val Acc: 0.0556\n",
      "Epoch 002 | Val Acc: 0.1202\n",
      "Epoch 003 | Val Acc: 0.1903\n",
      "Epoch 004 | Val Acc: 0.2242\n",
      "Epoch 005 | Val Acc: 0.2790\n",
      "Epoch 006 | Val Acc: 0.3492\n",
      "Epoch 007 | Val Acc: 0.4137\n",
      "Epoch 008 | Val Acc: 0.4661\n",
      "Epoch 009 | Val Acc: 0.5081\n",
      "Epoch 010 | Val Acc: 0.5476\n",
      "Epoch 011 | Val Acc: 0.5871\n",
      "Epoch 012 | Val Acc: 0.6089\n",
      "Epoch 013 | Val Acc: 0.6290\n",
      "Epoch 014 | Val Acc: 0.6524\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 191\u001b[39m\n\u001b[32m    187\u001b[39m test_loader = DataLoader(test_ds, batch_size=\u001b[32m128\u001b[39m)\n\u001b[32m    189\u001b[39m model = FullModel().to(device)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m acc = \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminer\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    200\u001b[39m accs.append(acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mtrain_and_eval\u001b[39m\u001b[34m(model, train_loader, test_loader, metric_loss, miner, epochs, patience)\u001b[39m\n\u001b[32m    112\u001b[39m loss = (\n\u001b[32m    113\u001b[39m     ce_loss(logits, y)\n\u001b[32m    114\u001b[39m     + \u001b[32m0.1\u001b[39m * metric_loss(z, y, indices_tuple)\n\u001b[32m    115\u001b[39m )\n\u001b[32m    117\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m    120\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\winterIntern project\\.venv_gpu\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\winterIntern project\\.venv_gpu\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\winterIntern project\\.venv_gpu\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_metric_learning import losses, miners, distances\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ---------------- DATASET ----------------\n",
    "class EMGDataset(Dataset):\n",
    "    def __init__(self, x_path, y_path):\n",
    "        X = np.load(x_path)\n",
    "        y = np.argmax(np.load(y_path), axis=1)\n",
    "\n",
    "        # channel-wise normalization (CRITICAL)\n",
    "        for i in range(X.shape[0]):\n",
    "            for c in range(6):\n",
    "                X[i, :, c] = (X[i, :, c] - X[i, :, c].mean()) / (X[i, :, c].std() + 1e-8)\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# ---------------- CNN + BiLSTM ENCODER ----------------\n",
    "class CNNBiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(6, 64, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "\n",
    "            nn.Conv1d(64, 256, kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=128,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)      # (B, C, T)\n",
    "        x = self.cnn(x)             # (B, 256, T')\n",
    "        x = x.permute(0, 2, 1)      # (B, T', 256)\n",
    "\n",
    "        out, _ = self.bilstm(x)\n",
    "        feat = out[:, -1, :]        # last timestep\n",
    "\n",
    "        z = self.fc(feat)\n",
    "        return z\n",
    "\n",
    "# ---------------- FULL MODEL ----------------\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, emb_dim=256, num_classes=62):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNBiLSTMEncoder(emb_dim)\n",
    "        self.classifier = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        logits = self.classifier(z)\n",
    "        return z, logits\n",
    "\n",
    "# ---------------- TRAIN + EVAL ----------------\n",
    "def train_and_eval(model, train_loader, test_loader,\n",
    "                   metric_loss, miner,\n",
    "                   epochs=300, patience=12):\n",
    "\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "    best_acc = -1\n",
    "    patience_ctr = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            z, logits = model(x)\n",
    "\n",
    "            #  CRITICAL FIX #1\n",
    "            z = F.normalize(z, dim=1)\n",
    "\n",
    "            #  CRITICAL FIX #2\n",
    "            indices_tuple = miner(z, y)\n",
    "\n",
    "            #  CRITICAL FIX #3 ( = 0.1)\n",
    "            loss = (\n",
    "                ce_loss(logits, y)\n",
    "                + 0.1 * metric_loss(z, y, indices_tuple)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _, logits = model(x)\n",
    "                preds = logits.argmax(1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1:03d} | Val Acc: {acc:.4f}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_acc\n",
    "\n",
    "encoder_losses = {\n",
    "    \"Triplet_L2\": losses.TripletMarginLoss(\n",
    "        margin=0.2,\n",
    "        distance=distances.LpDistance(p=2),\n",
    "        smooth_loss=True\n",
    "    ),\n",
    "    \"Triplet_Cosine\": losses.TripletMarginLoss(\n",
    "        margin=0.1,\n",
    "        distance=distances.CosineSimilarity(),\n",
    "        smooth_loss=True\n",
    "    ),\n",
    "    \"NTXent\": losses.NTXentLoss(temperature=0.2)\n",
    "}\n",
    "\n",
    "miner = miners.BatchEasyHardMiner(\n",
    "    pos_strategy=\"easy\",\n",
    "    neg_strategy=\"semihard\"\n",
    ")\n",
    "\n",
    "BASE = \"models/Data/Data/62_classes/UserDependenet\"\n",
    "\n",
    "final_results = {}\n",
    "\n",
    "for loss_name, metric_loss in encoder_losses.items():\n",
    "    print(f\"\\n========== {loss_name} ==========\")\n",
    "    accs = []\n",
    "\n",
    "    for split in range(1, 20):\n",
    "        train_ds = EMGDataset(\n",
    "            f\"{BASE}/Train/X_train_{split}.npy\",\n",
    "            f\"{BASE}/Train/y_train_{split}.npy\"\n",
    "        )\n",
    "        test_ds = EMGDataset(\n",
    "            f\"{BASE}/Test/X_test_{split}.npy\",\n",
    "            f\"{BASE}/Test/y_test_{split}.npy\"\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=128)\n",
    "\n",
    "        model = FullModel().to(device)\n",
    "\n",
    "        acc = train_and_eval(\n",
    "            model,\n",
    "            train_loader,\n",
    "            test_loader,\n",
    "            metric_loss,\n",
    "            miner\n",
    "        )\n",
    "\n",
    "        print(f\"Split {split} Acc: {acc:.4f}\")\n",
    "        accs.append(acc)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    final_results[loss_name] = np.mean(accs)\n",
    "\n",
    "print(\"\\n===== FINAL RESULTS =====\")\n",
    "for k, v in final_results.items():\n",
    "    print(f\"{k:20s}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (winter_gpu)",
   "language": "python",
   "name": "winter_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
